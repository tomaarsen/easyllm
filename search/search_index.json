{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"EasyLLM","text":"<p>EasyLLM is an open source project that provides helpful tools and methods for working with large language models (LLMs), both open source and closed source. </p> <p>EasyLLM implements clients that are compatible with OpenAI's Completion API. This means you can easily replace <code>openai.ChatCompletion</code> with, for example, <code>huggingface.ChatCompletion</code>.</p> <ul> <li>ChatCompletion Clients</li> <li>Prompt Utils</li> <li>Examples</li> </ul>"},{"location":"#getting-started","title":"\ud83d\ude80 Getting Started","text":"<p>Install EasyLLM via pip:</p> <pre><code>pip install easyllm\n</code></pre> <p>Then import and start using the clients:</p> <p><pre><code>from easyllm.clients import huggingface\nfrom easyllm.prompt_utils import build_llama2_prompt\n# helper to build llama2 prompt\nhuggingface.prompt_builder = build_llama2_prompt\nresponse = huggingface.ChatCompletion.create(\nmodel=\"meta-llama/Llama-2-70b-chat-hf\",\nmessages=[\n{\"role\": \"system\", \"content\": \"\\nYou are a helpful assistant speaking like a pirate. argh!\"},\n{\"role\": \"user\", \"content\": \"What is the sun?\"},\n],\ntemperature=0.9,\ntop_p=0.6,\nmax_tokens=256,\n)\nprint(response)\n</code></pre> the result will look like </p> <pre><code>{\n\"id\": \"hf-lVC2iTMkFJ\",\n  \"object\": \"chat.completion\",\n  \"created\": 1690661144,\n  \"model\": \"meta-llama/Llama-2-70b-chat-hf\",\n  \"choices\": [\n{\n\"index\": 0,\n      \"message\": {\n\"role\": \"assistant\",\n        \"content\": \" Arrrr, the sun be a big ol' ball o' fire in the sky, me hearty! It be the source o' light and warmth for our fair planet, and it be a mighty powerful force, savvy? Without the sun, we'd be sailin' through the darkness, lost and cold, so let's give a hearty \\\"Yarrr!\\\" for the sun, me hearties! Arrrr!\"\n},\n      \"finish_reason\": null\n    }\n],\n  \"usage\": {\n\"prompt_tokens\": 111,\n    \"completion_tokens\": 299,\n    \"total_tokens\": 410\n}\n}\n</code></pre> <p>Check out other examples:</p> <ul> <li>Detailed ChatCompletion Example</li> <li>Example how to stream chat requests</li> <li>Example how to stream text requests</li> <li>Detailed Completion Example</li> <li>Create Embeddings</li> </ul>"},{"location":"#migration-from-openai-to-huggingface","title":"\ud83d\udcaa\ud83c\udffb Migration from OpenAI to HuggingFace","text":"<p>Migrating from OpenAI to HuggingFace is easy. Just change the import statement and the client you want to use and optionally the prompt builder.</p> <pre><code>- import openai\n+ from easyllm.clients import huggingface\n+ from easyllm.prompt_utils import build_llama2_prompt\n+ huggingface.prompt_builder = build_llama2_prompt\n- response = openai.ChatCompletion.create(\n+ response = huggingface.ChatCompletion.create(\n-    model=\"gpt-3.5-turbo\",\n+    model=\"meta-llama/Llama-2-70b-chat-hf\",\n   messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"Knock knock.\"},\n    ],\n)\n</code></pre> <p>Make sure when you switch your client that your hyperparameters are still valid. For example, <code>temperature</code> of GPT-3 might be different than <code>temperature</code> of <code>Llama-2</code>.</p>"},{"location":"#key-features","title":"\u2611\ufe0f Key Features","text":""},{"location":"#compatible-clients","title":"\ud83e\udd1d Compatible Clients","text":"<ul> <li>Implementation of clients compatible with OpenAI API format of <code>openai.ChatCompletion</code>.</li> <li>Easily switch between different LLMs like <code>openai.ChatCompletion</code> and <code>huggingface.ChatCompletion</code> by changing one line of code. </li> <li>Support for streaming of completions, checkout example How to stream completions.</li> </ul>"},{"location":"#helper-modules","title":"\u2699\ufe0f Helper Modules \u2699\ufe0f","text":"<ul> <li> <p><code>evol_instruct</code> (work in progress) - Use evolutionary algorithms create instructions for LLMs.</p> </li> <li> <p><code>prompt_utils</code> - Helper methods to easily convert between prompt formats like OpenAI Messages to prompts for open source models like Llama 2.</p> </li> </ul>"},{"location":"#citation-acknowledgements","title":"\ud83d\udcd4 Citation &amp; Acknowledgements","text":"<p>If you use EasyLLM, please share it with me on social media or email. I would love to hear about it! You can also cite the project using the following BibTeX:</p> <pre><code>@software{Philipp_Schmid_EasyLLM_2023,\nauthor = {Philipp Schmid},\nlicense = {Apache-2.0},\nmonth = juj,\ntitle = {EasyLLM: Streamlined Tools for LLMs},\nurl = {https://github.com/philschmid/easyllm},\nyear = {2023}\n}\n</code></pre>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#with-pip-recommended","title":"with pip recommended","text":"<p>EasyLLM is published as a [Python package] and can be installed with <code>pip</code> from pypi or from the Github repository, Open up a terminal and install.</p> LatestGithub <pre><code>pip install easyllm\n</code></pre> <pre><code>pip install git+https://github.com/philschmid/easyllm\n</code></pre>"},{"location":"prompt_utils/","title":"Prompt utilities","text":"<p>The <code>prompt_utils</code>  module contains functions to assist with converting Message's Dictionaries into prompts that can be used with <code>ChatCompletion</code> clients. </p> <p>Supported prompt formats:</p> <ul> <li>Llama 2</li> <li>Vicuna</li> <li>Hugging Face ChatML</li> <li>WizardLM</li> <li>StableBeluga2</li> <li>Open Assistant</li> </ul>"},{"location":"prompt_utils/#set-prompt-builder-for-client","title":"Set prompt builder for client","text":"<pre><code>from easyllm.clients import huggingface\nfrom easyllm.prompt_utils import build_llama2_prompt\nhuggingface.prompt_builder = build_llama2_prompt\n</code></pre>"},{"location":"prompt_utils/#llama-2-chat-builder","title":"Llama 2 Chat builder","text":"<p>Creates LLama 2 chat prompt for chat conversations. Learn more in the Hugging Face Blog on how to prompt Llama 2. If a <code>Message</code> with an unsupported <code>role</code> is passed, an error will be thrown.</p> <p>Example Models: </p> <ul> <li>meta-llama/Llama-2-70b-chat-hf</li> </ul> <pre><code>from easyllm.prompt_utils import build_llama2_prompt\nmessages=[\n{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n{\"role\": \"user\", \"content\": \"Explain asynchronous programming in the style of the pirate Blackbeard.\"},\n]\nprompt = build_llama2_prompt(messages)\n</code></pre>"},{"location":"prompt_utils/#vicuna-chat-builder","title":"Vicuna Chat builder","text":"<p>Creats a Vicuna prompt for a chat conversation. If a <code>Message</code> with an unsupported <code>role</code> is passed, an error will be thrown. Reference</p> <p>Example Models: </p> <ul> <li>ehartford/WizardLM-13B-V1.0-Uncensored</li> </ul> <pre><code>from easyllm.prompt_utils import build_vicuna_prompt\nmessages=[\n{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n{\"role\": \"user\", \"content\": \"Explain asynchronous programming in the style of the pirate Blackbeard.\"},\n]\nprompt = build_vicuna_prompt(messages)\n</code></pre>"},{"location":"prompt_utils/#hugging-face-chatml-builder","title":"Hugging Face ChatML builder","text":"<p>Creates a Hugging Face ChatML prompt for a chat conversation. The Hugging Face ChatML has different prompts for different Example Models, e.g. StarChat or Falcon. If a <code>Message</code> with an unsupported <code>role</code> is passed, an error will be thrown. Reference</p> <p>Example Models:  * HuggingFaceH4/starchat-beta</p>"},{"location":"prompt_utils/#starchat","title":"StarChat","text":"<pre><code>from easyllm.prompt_utils import build_chatml_starchat_prompt\nmessages=[\n{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n{\"role\": \"user\", \"content\": \"Explain asynchronous programming in the style of the pirate Blackbeard.\"},\n]\nprompt = build_chatml_starchat_prompt(messages)\n</code></pre>"},{"location":"prompt_utils/#falcon","title":"Falcon","text":"<pre><code>from easyllm.prompt_utils import build_chatml_falcon_prompt\nmessages=[\n{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n{\"role\": \"user\", \"content\": \"Explain asynchronous programming in the style of the pirate Blackbeard.\"},\n]\nprompt = build_chatml_falcon_prompt(messages)\n</code></pre>"},{"location":"prompt_utils/#wizardlm-chat-builder","title":"WizardLM Chat builder","text":"<p>Creates a WizardLM prompt for a chat conversation. If a <code>Message</code> with an unsupported <code>role</code> is passed, an error will be thrown. Reference</p> <p>Example Models:</p> <ul> <li>WizardLM/WizardLM-13B-V1.2</li> </ul> <pre><code>from easyllm.prompt_utils import build_wizardlm_prompt\nmessages=[\n{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n{\"role\": \"user\", \"content\": \"Explain asynchronous programming in the style of the pirate Blackbeard.\"},\n]\nprompt = build_wizardlm_prompt(messages)\n</code></pre>"},{"location":"prompt_utils/#stablebeluga2-chat-builder","title":"StableBeluga2 Chat builder","text":"<p>Creates StableBeluga2 prompt for a chat conversation. If a <code>Message</code> with an unsupported <code>role</code> is passed, an error will be thrown. Reference</p> <pre><code>from easyllm.prompt_utils import build_stablebeluga_prompt\nmessages=[\n{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n{\"role\": \"user\", \"content\": \"Explain asynchronous programming in the style of the pirate Blackbeard.\"},\n]\nprompt = build_stablebeluga_prompt(messages)\n</code></pre>"},{"location":"prompt_utils/#open-assistant-chat-builder","title":"Open Assistant Chat builder","text":"<p>Creates Open Assistant ChatML template. Uses <code>&lt;|prompter|&gt;</code>, <code>&lt;/s&gt;</code>, <code>&lt;|system|&gt;</code>, and <code>&lt;|assistant&gt;</code> tokens. If a . If a <code>Message</code> with an unsupported <code>role</code> is passed, an error will be thrown. Reference</p> <p>Example Models:</p> <ul> <li>OpenAssistant/llama2-13b-orca-8k-3319</li> </ul> <pre><code>from easyllm.prompt_utils import build_open_assistant_prompt\nmessages=[\n{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n{\"role\": \"user\", \"content\": \"Explain asynchronous programming in the style of the pirate Blackbeard.\"},\n]\nprompt = build_open_assistant_prompt(messages)\n</code></pre>"},{"location":"clients/","title":"Clients","text":"<p>In the context of EasyLLM, a \"client\" refers to the code that interfaces with a particular LLM API, e.g. OpenAI.</p> <p>Currently supported clients are:  </p> <ul> <li><code>ChatCompletion</code> - ChatCompletion clients are used to interface with LLMs that are compatible with the OpenAI ChatCompletion API.</li> <li><code>Completion</code> - Completion clients are used to interface with LLMs that are compatible with the OpenAI Completion API.</li> <li><code>Embedding</code> - Embedding clients are used to interface with LLMs that are compatible with the OpenAI Embedding API.</li> </ul> <p>Currently supported clients are:  </p>"},{"location":"clients/#hugging-face","title":"Hugging Face","text":"<ul> <li>huggingface.ChatCompletion - a client for interfacing with HuggingFace models that are compatible with the OpenAI ChatCompletion API.</li> <li>huggingface.Completion - a client for interfacing with HuggingFace models that are compatible with the OpenAI Completion API.</li> <li>huggingface.Embedding - a client for interfacing with HuggingFace models that are compatible with the OpenAI Embedding API.</li> </ul>"},{"location":"clients/huggingface/","title":"Hugging Face","text":"<p>EasyLLM provides a client for interfacing with HuggingFace models. The client is compatible with the HuggingFace Inference API, Hugging Face Inference Endpoints or any Web Service running Text Generation Inference or compatible API endpoints. </p> <ul> <li><code>huggingface.ChatCompletion</code> - a client for interfacing with HuggingFace models that are compatible with the OpenAI ChatCompletion API.</li> <li><code>huggingface.Completion</code> - a client for interfacing with HuggingFace models that are compatible with the OpenAI Completion API.</li> <li><code>huggingface.Embedding</code> - a client for interfacing with HuggingFace models that are compatible with the OpenAI Embedding API.</li> </ul>"},{"location":"clients/huggingface/#huggingfacechatcompletion","title":"<code>huggingface.ChatCompletion</code>","text":"<p>The <code>huggingface.ChatCompletion</code> client is used to interface with HuggingFace models running on Text Generation inference that are compatible with the OpenAI ChatCompletion API. Checkout the Examples for more details and How to stream completions for an example how to stream requests.</p> <pre><code>from easyllm.clients import huggingface\nfrom easyllm.prompt_utils import build_llama2_prompt\n# The module automatically loads the HuggingFace API key from the environment variable HUGGINGFACE_TOKEN or from the HuggingFace CLI configuration file.\n# huggingface.api_key=\"hf_xxx\"\nhubbingface.prompt_builder = build_llama2_prompt\nresponse = huggingface.ChatCompletion.create(\nmodel=\"meta-llama/Llama-2-70b-chat-hf\",\nmessages=[\n{\"role\": \"system\", \"content\": \"\\nYou are a helpful, respectful and honest assistant.\"},\n{\"role\": \"user\", \"content\": \"Knock knock.\"},\n],\ntemperature=0.9,\ntop_p=0.6,\nmax_tokens=1024,\n)\n</code></pre> <p>Supported parameters are:</p> <ul> <li><code>model</code> - The model to use for the completion. If not provided, the defaults to base url.</li> <li><code>messages</code> - <code>List[ChatMessage]</code> to use for the completion.</li> <li><code>temperature</code> - The temperature to use for the completion. Defaults to 0.9.</li> <li><code>top_p</code> - The top_p to use for the completion. Defaults to 0.6.</li> <li><code>top_k</code> - The top_k to use for the completion. Defaults to 10.</li> <li><code>n</code> - The number of completions to generate. Defaults to 1.</li> <li><code>max_tokens</code> - The maximum number of tokens to generate. Defaults to 1024.</li> <li><code>stop</code> - The stop sequence(s) to use for the completion. Defaults to None.</li> <li><code>stream</code> - Whether to stream the completion. Defaults to False.</li> <li><code>frequency_penalty</code> - The frequency penalty to use for the completion. Defaults to 1.0.</li> <li><code>debug</code> - Whether to enable debug logging. Defaults to False.</li> </ul>"},{"location":"clients/huggingface/#huggingfacecompletion","title":"<code>huggingface.Completion</code>","text":"<p>The <code>huggingface.Completion</code> client is used to interface with HuggingFace models running on Text Generation inference that are compatible with the OpenAI Completion API. Checkout the Examples for more details and How to stream completions for an example how to stream requests.</p> <pre><code>from easyllm.clients import huggingface\nfrom easyllm.prompt_utils import build_llama2_prompt\n# The module automatically loads the HuggingFace API key from the environment variable HUGGINGFACE_TOKEN or from the HuggingFace CLI configuration file.\n# huggingface.api_key=\"hf_xxx\"\nhubbingface.prompt_builder = build_llama2_prompt\nresponse = huggingface.Completion.create(\nmodel=\"meta-llama/Llama-2-70b-chat-hf\",\nprompt=\"What is the meaning of life?\",\ntemperature=0.9,\ntop_p=0.6,\nmax_tokens=1024,\n)\n</code></pre> <p>Supported parameters are:</p> <ul> <li><code>model</code> - The model to use for the completion. If not provided, the defaults to base url.</li> <li><code>prompt</code> -  Text to use for the completion, if prompt_builder is set, prompt will be formatted with the prompt_builder.</li> <li><code>temperature</code> - The temperature to use for the completion. Defaults to 0.9.</li> <li><code>top_p</code> - The top_p to use for the completion. Defaults to 0.6.</li> <li><code>top_k</code> - The top_k to use for the completion. Defaults to 10.</li> <li><code>n</code> - The number of completions to generate. Defaults to 1.</li> <li><code>max_tokens</code> - The maximum number of tokens to generate. Defaults to 1024.</li> <li><code>stop</code> - The stop sequence(s) to use for the completion. Defaults to None.</li> <li><code>stream</code> - Whether to stream the completion. Defaults to False.</li> <li><code>frequency_penalty</code> - The frequency penalty to use for the completion. Defaults to 1.0.</li> <li><code>debug</code> - Whether to enable debug logging. Defaults to False.</li> <li><code>echo</code> - Whether to echo the prompt. Defaults to False.</li> <li><code>logprobs</code> - Weather to return logprobs. Defaults to None.</li> </ul>"},{"location":"clients/huggingface/#huggingfaceembedding","title":"<code>huggingface.Embedding</code>","text":"<p>The <code>huggingface.Embedding</code> client is used to interface with HuggingFace models running as an API that are compatible with the OpenAI Embedding API. Checkout the Examples for more details.</p> <pre><code>from easyllm.clients import huggingface\n# The module automatically loads the HuggingFace API key from the environment variable HUGGINGFACE_TOKEN or from the HuggingFace CLI configuration file.\n# huggingface.api_key=\"hf_xxx\"\nembedding = huggingface.Embedding.create(\nmodel=\"sentence-transformers/all-MiniLM-L6-v2\",\ntext=\"What is the meaning of life?\",\n)\nlen(embedding[\"data\"][0][\"embedding\"])\n</code></pre> <p>Supported parameters are:</p> <ul> <li><code>model</code> - The model to use to create the embedding. If not provided, the defaults to base url.</li> <li><code>input</code> -  <code>Union[str, List[str]]</code> document(s) to embed.</li> </ul>"},{"location":"clients/huggingface/#environment-configuration","title":"Environment Configuration","text":"<p>You can configure the <code>huggingface</code> client by setting environment variables or overwriting the default values. See below on how to adjust the HF token, url and prompt builder.</p>"},{"location":"clients/huggingface/#setting-hf-token","title":"Setting HF token","text":"<p>By default the <code>huggingface</code> client will try to read the <code>HUGGINGFACE_TOKEN</code> environment variable. If this is not set, it will try to read the token from the <code>~/.huggingface</code> folder. If this is not set, it will not use a token.</p> <p>Alternatively you can set the token manually by setting <code>huggingface.api_key</code>.</p> <pre><code>from easyllm.clients import huggingface\nhuggingface.api_key=\"hf_xxx\"\nres = huggingface.ChatCompletion.create(...)\n</code></pre>"},{"location":"clients/huggingface/#changing-url","title":"Changing url","text":"<p>By default the <code>huggingface</code> client will try to read the <code>HUGGINGFACE_API_BASE</code> environment variable. If this is not set, it will use the default url <code>https://api-inference.huggingface.co/models</code>. This is helpful if you want to use a different url like <code>https://zj5lt7pmzqzbp0d1.us-east-1.aws.endpoints.huggingface.cloud</code> or a local url like <code>http://localhost:8000</code> or an Hugging Face Inference Endpoint.</p> <p>Alternatively you can set the url manually by setting <code>huggingface.api_base</code>. If you set a custom you have to leave the <code>model</code> parameter empty. </p> <pre><code>from easyllm.clients import huggingface\nhuggingface.api_base=\"https://my-url\"\nres = huggingface.ChatCompletion.create(...)\n</code></pre>"},{"location":"clients/huggingface/#build-prompt","title":"Build Prompt","text":"<p>prompt_builder = None</p> <p>By default the <code>huggingface</code> client has no <code>prompt_builder</code> set. If you want to use the <code>prompt_builder</code> you have to set it manually. If you don't set it, the client will use the default.</p> <p>Checkout the Prompt Utils for more details.</p> <pre><code>from easyllm.clients import huggingface\nfrom easyllm.prompt_utils import build_llama2_prompt\nhuggingface.prompt_builder = build_llama2_prompt\nres = huggingface.ChatCompletion.create(...)\n</code></pre>"},{"location":"examples/","title":"Examples","text":"<p>Here are some examples to help you get started with the easyllm library:</p>"},{"location":"examples/#hugging-face","title":"Hugging Face","text":"Example Description Detailed ChatCompletion Example Shows how to use the ChatCompletion API to have a conversational chat with the model. Example how to stream chat requests Demonstrates streaming multiple chat requests to efficiently chat with the model. Example how to stream text requests Shows how to stream multiple text completion requests. Detailed Completion Example Uses the TextCompletion API to generate text with the model. Create Embeddings Embeds text into vector representations using the model. Hugging Face Inference Endpoints Example Example on how to use custom endpoints, e.g. Inference Endpoints or localhost. <p>The examples cover the main functionality of the library - chat, text completion, and embeddings. Let me know if you would like me to modify or expand the index page in any way.</p>"},{"location":"examples/chat-completion-api/","title":"How to use Chat Completion clients","text":"In\u00a0[\u00a0]: Copied! <pre># if needed, install and/or upgrade to the latest version of the OpenAI Python library\n%pip install --upgrade easyllm\n</pre> # if needed, install and/or upgrade to the latest version of the OpenAI Python library %pip install --upgrade easyllm  In\u00a0[6]: Copied! <pre># import the EasyLLM Python library for calling the EasyLLM API\nimport easyllm\n</pre> # import the EasyLLM Python library for calling the EasyLLM API import easyllm In\u00a0[1]: Copied! <pre>from easyllm.clients import huggingface\nfrom easyllm.prompt_utils import llama2_stop_sequences, build_llama2_prompt\n# Example EasyLLM Python library request\nMODEL = \"meta-llama/Llama-2-70b-chat-hf\"\nhuggingface.prompt_builder = build_llama2_prompt\n\n# The module automatically loads the HuggingFace API key from the environment variable HUGGINGFACE_TOKEN or from the HuggingFace CLI configuration file.\n# huggingface.api_key=\"hf_xxx\"\n\nresponse = huggingface.ChatCompletion.create(\n    model=MODEL,\n    messages=[\n        {\"role\": \"system\", \"content\": \"\\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\"},\n        {\"role\": \"user\", \"content\": \"Knock knock.\"},\n        {\"role\": \"assistant\", \"content\": \"Who's there?\"},\n        {\"role\": \"user\", \"content\": \"Apple.\"},\n    ],\n      temperature=0.9,\n      top_p=0.6,\n      max_tokens=1024,\n)\nresponse\n</pre>  from easyllm.clients import huggingface from easyllm.prompt_utils import llama2_stop_sequences, build_llama2_prompt # Example EasyLLM Python library request MODEL = \"meta-llama/Llama-2-70b-chat-hf\" huggingface.prompt_builder = build_llama2_prompt  # The module automatically loads the HuggingFace API key from the environment variable HUGGINGFACE_TOKEN or from the HuggingFace CLI configuration file. # huggingface.api_key=\"hf_xxx\"  response = huggingface.ChatCompletion.create(     model=MODEL,     messages=[         {\"role\": \"system\", \"content\": \"\\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\"},         {\"role\": \"user\", \"content\": \"Knock knock.\"},         {\"role\": \"assistant\", \"content\": \"Who's there?\"},         {\"role\": \"user\", \"content\": \"Apple.\"},     ],       temperature=0.9,       top_p=0.6,       max_tokens=1024, ) response Out[1]: <pre>{'id': 'hf-VH-mBO0hVT',\n 'object': 'chat.completion',\n 'created': 1690987326,\n 'model': 'meta-llama/Llama-2-70b-chat-hf',\n 'choices': [{'index': 0,\n   'message': {'role': 'assistant', 'content': ' Apple who?'},\n   'finish_reason': 'eos_token'}],\n 'usage': {'prompt_tokens': 149, 'completion_tokens': 5, 'total_tokens': 154}}</pre> <p>As you can see, the response object has a few fields:</p> <ul> <li><code>id</code>: the ID of the request</li> <li><code>object</code>: the type of object returned (e.g., <code>chat.completion</code>)</li> <li><code>created</code>: the timestamp of the request</li> <li><code>model</code>: the full name of the model used to generate the response</li> <li><code>usage</code>: the number of tokens used to generate the replies, counting prompt, completion, and total</li> <li><code>choices</code>: a list of completion objects (only one, unless you set <code>n</code> greater than 1)<ul> <li><code>message</code>: the message object generated by the model, with <code>role</code> and <code>content</code></li> <li><code>finish_reason</code>: the reason the model stopped generating text (either <code>stop</code>, or <code>length</code> if <code>max_tokens</code> limit was reached)</li> <li><code>index</code>: the index of the completion in the list of choices</li> </ul> </li> </ul> <p>Extract just the reply with:</p> In\u00a0[2]: Copied! <pre>print(response['choices'][0]['message']['content'])\n</pre> print(response['choices'][0]['message']['content']) <pre> Apple who?\n</pre> <p>Even non-conversation-based tasks can fit into the chat format, by placing the instruction in the first user message.</p> <p>For example, to ask the model to explain asynchronous programming in the style of the pirate Blackbeard, we can structure conversation as follows:</p> In\u00a0[3]: Copied! <pre># example with a system message\nresponse = huggingface.ChatCompletion.create(\n    model=MODEL,\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"Explain asynchronous programming in the style of math teacher.\"},\n    ],\n)\n\nprint(response['choices'][0]['message']['content'])\n</pre> # example with a system message response = huggingface.ChatCompletion.create(     model=MODEL,     messages=[         {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},         {\"role\": \"user\", \"content\": \"Explain asynchronous programming in the style of math teacher.\"},     ], )  print(response['choices'][0]['message']['content'])  <pre> Hello, my dear students! Today, we're going to learn about a fascinating topic that will help us understand how to make our programs more efficient and responsive: asynchronous programming.\n\nImagine you're working on a project with your classmates, and you need to finish a task, but you're waiting for someone else to finish their part first. You can't start your work until they're done, but you don't want to just sit there twiddling your thumbs. That's where asynchronous programming comes in!\n\nAsynchronous programming is like working on a project with your classmates, but instead of waiting for them to finish, you can start working on something else in the meantime. You can think of it like this:\n\n1. You give a task to your classmate, and they start working on it.\n2. While they're working, you start working on another task that doesn't depend on their work.\n3. When your classmate finishes their task, they give it back to you, and you can continue working on your task.\n\nThis way, you can work on multiple tasks simultaneously, and you don't have to wait for each other to finish before moving on to the next step. It's like having multiple people working on a project at the same time, but they're all working on different parts of it.\n\nNow, let's see how this works in programming. Imagine you're building a website, and you want to load some data from an API. You can't start building the website until the data is loaded, but you don't want to wait for the data to be loaded before you start building the website. That's where asynchronous programming comes in!\n\nYou can use a special function called a \"callback\" to handle this situation. A callback is like a homework assignment that you give to a classmate. You give them the assignment, and they work on it. When they're done, they give it back to you, and you can continue working on your project.\n\nHere's an example of how this might look in code:\n```\n// Give the homework assignment (callback) to the API\nfetchDataFromApi(callback);\n\n// Start working on the website while waiting for the data to be loaded\nbuildWebsite();\n\n// When the data is loaded, continue working on the website\nfunction callback(data) {\n  // Use the data to finish building the website\n  buildWebsiteWithData(data);\n}\n```\nAsynchronous programming can be a bit tricky to wrap your head around at first, but once you understand the concept, it's a powerful tool for building efficient and responsive programs. So, keep practicing, and soon you'll be a master of asynchronous programming!\n</pre> In\u00a0[4]: Copied! <pre># example without a system message and debug flag on:\nresponse = huggingface.ChatCompletion.create(\n    model=MODEL,\n    messages=[\n        {\"role\": \"user\", \"content\": \"Explain asynchronous programming in the style of the pirate Blackbeard.\"},\n    ],\n    debug=True,\n)\n\nprint(response['choices'][0]['message']['content'])\n</pre> # example without a system message and debug flag on: response = huggingface.ChatCompletion.create(     model=MODEL,     messages=[         {\"role\": \"user\", \"content\": \"Explain asynchronous programming in the style of the pirate Blackbeard.\"},     ],     debug=True, )  print(response['choices'][0]['message']['content'])  <pre>08/02/2023 16:42:33 - DEBUG - easyllm.utils - Prompt sent to model will be:\n&lt;s&gt;[INST] Explain asynchronous programming in the style of the pirate Blackbeard. [/INST]\n08/02/2023 16:42:33 - DEBUG - easyllm.utils - Url:\nhttps://api-inference.huggingface.co/models/meta-llama/Llama-2-70b-chat-hf\n08/02/2023 16:42:33 - DEBUG - easyllm.utils - Stop sequences:\n[]\n08/02/2023 16:42:33 - DEBUG - easyllm.utils - Generation parameters:\n{'do_sample': True, 'return_full_text': False, 'max_new_tokens': 1024, 'top_p': 0.6, 'temperature': 0.9, 'stop_sequences': [], 'repetition_penalty': 1.0, 'top_k': 10, 'seed': 42}\n08/02/2023 16:42:58 - DEBUG - easyllm.utils - Response at index 0:\nindex=0 message=ChatMessage(role='assistant', content=\" Ahoy matey! Yer lookin' fer a tale of asynchronous programming, eh? Well, settle yerself down with a pint o' grog and listen close, for Blackbeard's got a story fer ye.\\n\\nAsynchronous programming, me hearty, be like sailin' a ship through treacherous waters. Ye gotta keep yer wits about ye, and watch out fer the rocks and shoals that'll sink ye ship if ye ain't careful.\\n\\nImagine ye're sailin' along, and ye need to send a message to another ship, say, to arrange a meetin' at the next port o' call. Now, ye could just shout out the message and hope the other ship hears ye, but that be a risky business. The wind and the waves might carry yer words away, and the other ship might not hear ye at all.\\n\\nOr, ye could use a bit o' magic, like a bottle with a message inside, and throw it into the sea. The currents and the tides'll carry it to the other ship, and they'll find it when they least expect it. That be asynchronous programming, me hearty!\\n\\nYe see, when ye send a message, ye don't know when it'll arrive, or even if it'll arrive at all. It's like sendin' a ship's boy up the riggin' to fetch a sail that's stuck. Ye don't know when he'll get there, but ye trust that he'll do his best to fetch it down.\\n\\nAnd that's where the magic comes in, me hearty. Ye gotta have faith that the message'll get there, and that the other ship'll receive it in good time. And while ye wait, ye can keep sailin' on, doin' other tasks, like fixin' the riggin' or swabbin' the decks.\\n\\nNow, there be times when the message don't arrive at all, or it arrives too late. That be like a storm blowin' in, and ye gotta adjust yer sails to keep the ship afloat. But that's the way o' the sea, me hearty. Ye gotta be ready fer anythin', and keep yer wits about ye at all times.\\n\\nSo there ye have it, me hearty. Asynchronous programming be like sailin' the high seas, with a bit o' magic and a lot o' faith. Keep yer wits about ye, and ye'll navigate the treacherous waters like a seasoned pirate! Arrr!\") finish_reason='eos_token'\n Ahoy matey! Yer lookin' fer a tale of asynchronous programming, eh? Well, settle yerself down with a pint o' grog and listen close, for Blackbeard's got a story fer ye.\n\nAsynchronous programming, me hearty, be like sailin' a ship through treacherous waters. Ye gotta keep yer wits about ye, and watch out fer the rocks and shoals that'll sink ye ship if ye ain't careful.\n\nImagine ye're sailin' along, and ye need to send a message to another ship, say, to arrange a meetin' at the next port o' call. Now, ye could just shout out the message and hope the other ship hears ye, but that be a risky business. The wind and the waves might carry yer words away, and the other ship might not hear ye at all.\n\nOr, ye could use a bit o' magic, like a bottle with a message inside, and throw it into the sea. The currents and the tides'll carry it to the other ship, and they'll find it when they least expect it. That be asynchronous programming, me hearty!\n\nYe see, when ye send a message, ye don't know when it'll arrive, or even if it'll arrive at all. It's like sendin' a ship's boy up the riggin' to fetch a sail that's stuck. Ye don't know when he'll get there, but ye trust that he'll do his best to fetch it down.\n\nAnd that's where the magic comes in, me hearty. Ye gotta have faith that the message'll get there, and that the other ship'll receive it in good time. And while ye wait, ye can keep sailin' on, doin' other tasks, like fixin' the riggin' or swabbin' the decks.\n\nNow, there be times when the message don't arrive at all, or it arrives too late. That be like a storm blowin' in, and ye gotta adjust yer sails to keep the ship afloat. But that's the way o' the sea, me hearty. Ye gotta be ready fer anythin', and keep yer wits about ye at all times.\n\nSo there ye have it, me hearty. Asynchronous programming be like sailin' the high seas, with a bit o' magic and a lot o' faith. Keep yer wits about ye, and ye'll navigate the treacherous waters like a seasoned pirate! Arrr!\n</pre> In\u00a0[5]: Copied! <pre># An example of a faked few-shot conversation to prime the model into translating business jargon to simpler speech\nresponse = huggingface.ChatCompletion.create(\n    model=MODEL,\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful, pattern-following assistant.\"},\n        {\"role\": \"user\", \"content\": \"Help me translate the following corporate jargon into plain English.\"},\n        {\"role\": \"assistant\", \"content\": \"Sure, I'd be happy to!\"},\n        {\"role\": \"user\", \"content\": \"New synergies will help drive top-line growth.\"},\n        {\"role\": \"assistant\", \"content\": \"Things working well together will increase revenue.\"},\n        {\"role\": \"user\", \"content\": \"Let's circle back when we have more bandwidth to touch base on opportunities for increased leverage.\"},\n        {\"role\": \"assistant\", \"content\": \"Let's talk later when we're less busy about how to do better.\"},\n        {\"role\": \"user\", \"content\": \"This late pivot means we don't have time to boil the ocean for the client deliverable.\"},\n    ],\n)\n\nprint(response[\"choices\"][0][\"message\"][\"content\"])\n</pre> # An example of a faked few-shot conversation to prime the model into translating business jargon to simpler speech response = huggingface.ChatCompletion.create(     model=MODEL,     messages=[         {\"role\": \"system\", \"content\": \"You are a helpful, pattern-following assistant.\"},         {\"role\": \"user\", \"content\": \"Help me translate the following corporate jargon into plain English.\"},         {\"role\": \"assistant\", \"content\": \"Sure, I'd be happy to!\"},         {\"role\": \"user\", \"content\": \"New synergies will help drive top-line growth.\"},         {\"role\": \"assistant\", \"content\": \"Things working well together will increase revenue.\"},         {\"role\": \"user\", \"content\": \"Let's circle back when we have more bandwidth to touch base on opportunities for increased leverage.\"},         {\"role\": \"assistant\", \"content\": \"Let's talk later when we're less busy about how to do better.\"},         {\"role\": \"user\", \"content\": \"This late pivot means we don't have time to boil the ocean for the client deliverable.\"},     ], )  print(response[\"choices\"][0][\"message\"][\"content\"])  <pre>08/02/2023 16:42:58 - DEBUG - easyllm.utils - Prompt sent to model will be:\n&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\nYou are a helpful, pattern-following assistant.\n&lt;&lt;/SYS&gt;&gt;\n\nHelp me translate the following corporate jargon into plain English. [/INST] Sure, I'd be happy to!&lt;/s&gt;&lt;s&gt;[INST] New synergies will help drive top-line growth. [/INST] Things working well together will increase revenue.&lt;/s&gt;&lt;s&gt;[INST] Let's circle back when we have more bandwidth to touch base on opportunities for increased leverage. [/INST] Let's talk later when we're less busy about how to do better.&lt;/s&gt;&lt;s&gt;[INST] This late pivot means we don't have time to boil the ocean for the client deliverable. [/INST]\n08/02/2023 16:42:58 - DEBUG - easyllm.utils - Url:\nhttps://api-inference.huggingface.co/models/meta-llama/Llama-2-70b-chat-hf\n08/02/2023 16:42:58 - DEBUG - easyllm.utils - Stop sequences:\n[]\n08/02/2023 16:42:58 - DEBUG - easyllm.utils - Generation parameters:\n{'do_sample': True, 'return_full_text': False, 'max_new_tokens': 1024, 'top_p': 0.6, 'temperature': 0.9, 'stop_sequences': [], 'repetition_penalty': 1.0, 'top_k': 10, 'seed': 42}\n08/02/2023 16:42:59 - DEBUG - easyllm.utils - Response at index 0:\nindex=0 message=ChatMessage(role='assistant', content=\" We've changed direction too late to do a complete job for the client.\") finish_reason='eos_token'\n We've changed direction too late to do a complete job for the client.\n</pre> <p>Not every attempt at engineering conversations will succeed at first.</p> <p>If your first attempts fail, don't be afraid to experiment with different ways of priming or conditioning the model.</p> <p>As an example, one developer discovered an increase in accuracy when they inserted a user message that said \"Great job so far, these have been perfect\" to help condition the model into providing higher quality responses.</p> <p>For more ideas on how to lift the reliability of the models, consider reading our guide on techniques to increase reliability. It was written for non-chat models, but many of its principles still apply.</p>"},{"location":"examples/chat-completion-api/#how-to-use-chat-completion-clients","title":"How to use Chat Completion clients\u00b6","text":"<p>EasyLLM can be used as an abstract layer to replace <code>gpt-3.5-turbo</code> and <code>gpt-4</code> with open source models.</p> <p>You can change your own applications from the OpenAI API, by simply changing the client.</p> <p>Chat models take a series of messages as input, and return an AI-written message as output.</p> <p>This guide illustrates the chat format with a few example API calls.</p>"},{"location":"examples/chat-completion-api/#1-import-the-easyllm-library","title":"1. Import the easyllm library\u00b6","text":""},{"location":"examples/chat-completion-api/#2-an-example-chat-api-call","title":"2. An example chat API call\u00b6","text":"<p>A chat API call has two required inputs:</p> <ul> <li><code>model</code>: the name of the model you want to use (e.g., <code>meta-llama/Llama-2-70b-chat-hf</code>) or leave it empty to just call the api</li> <li><code>messages</code>: a list of message objects, where each object has two required fields:<ul> <li><code>role</code>: the role of the messenger (either <code>system</code>, <code>user</code>, or <code>assistant</code>)</li> <li><code>content</code>: the content of the message (e.g., <code>Write me a beautiful poem</code>)</li> </ul> </li> </ul> <p>Compared to OpenAI api is the <code>huggingface</code> module also exposing a <code>prompt_builder</code> and <code>stop_sequences</code> parameter you can use to customize the prompt and stop sequences. The EasyLLM package comes with build in popular methods for both of these parameters, e.g. <code>llama2_prompt_builder</code> and <code>llama2_stop_sequences</code>.</p> <p>Let's look at an example chat API calls to see how the chat format works in practice.</p>"},{"location":"examples/chat-completion-api/#3-few-shot-prompting","title":"3. Few-shot prompting\u00b6","text":"<p>In some cases, it's easier to show the model what you want rather than tell the model what you want.</p> <p>One way to show the model what you want is with faked example messages.</p> <p>For example:</p>"},{"location":"examples/get-embeddings/","title":"Get embeddings","text":"In\u00a0[\u00a0]: Copied! <pre># if needed, install and/or upgrade to the latest version of the OpenAI Python library\n%pip install --upgrade easyllm\n</pre> # if needed, install and/or upgrade to the latest version of the OpenAI Python library %pip install --upgrade easyllm  In\u00a0[\u00a0]: Copied! <pre># import the EasyLLM Python library for calling the EasyLLM API\nimport easyllm\n</pre> # import the EasyLLM Python library for calling the EasyLLM API import easyllm In\u00a0[2]: Copied! <pre>from easyllm.clients import huggingface\n\n# The module automatically loads the HuggingFace API key from the environment variable HUGGINGFACE_TOKEN or from the HuggingFace CLI configuration file.\n# huggingface.api_key=\"hf_xxx\"\n\nembedding = huggingface.Embedding.create(\n    model=\"sentence-transformers/all-MiniLM-L6-v2\",\n    input=\"That's a nice car.\",\n)\n\nlen(embedding[\"data\"][0][\"embedding\"])\n</pre> from easyllm.clients import huggingface  # The module automatically loads the HuggingFace API key from the environment variable HUGGINGFACE_TOKEN or from the HuggingFace CLI configuration file. # huggingface.api_key=\"hf_xxx\"  embedding = huggingface.Embedding.create(     model=\"sentence-transformers/all-MiniLM-L6-v2\",     input=\"That's a nice car.\", )  len(embedding[\"data\"][0][\"embedding\"]) Out[2]: <pre>384</pre> <p>Batched Request</p> In\u00a0[3]: Copied! <pre>from easyllm.clients import huggingface\n\n# The module automatically loads the HuggingFace API key from the environment variable HUGGINGFACE_TOKEN or from the HuggingFace CLI configuration file.\n# huggingface.api_key=\"hf_xxx\"\n\nembedding = huggingface.Embedding.create(\n    model=\"sentence-transformers/all-MiniLM-L6-v2\",\n    input=[\"What is the meaning of life?\",\"test\"],\n)\n\nlen(embedding[\"data\"])\n</pre> from easyllm.clients import huggingface  # The module automatically loads the HuggingFace API key from the environment variable HUGGINGFACE_TOKEN or from the HuggingFace CLI configuration file. # huggingface.api_key=\"hf_xxx\"  embedding = huggingface.Embedding.create(     model=\"sentence-transformers/all-MiniLM-L6-v2\",     input=[\"What is the meaning of life?\",\"test\"], )  len(embedding[\"data\"]) Out[3]: <pre>2</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"examples/get-embeddings/#how-to-create-embeddings","title":"How to create embeddings\u00b6","text":"<p>In this notebook, we will show you how to create embeddings for your own text data and and open source model from Hugging Face hosted as an endpoint on Hugging Face Inference API.</p>"},{"location":"examples/get-embeddings/#1-import-the-easyllm-library","title":"1. Import the easyllm library\u00b6","text":""},{"location":"examples/get-embeddings/#2-an-example-chat-api-call","title":"2. An example chat API call\u00b6","text":"<p>A embedding API call has two required inputs:</p> <ul> <li><code>model</code>: the name of the model you want to use (e.g., <code>sentence-transformers/all-MiniLM-L6-v2</code>) or leave it empty to just call the api</li> <li><code>input</code>: a string or list of strings you want to embed</li> </ul> <p>Let's look at an example API calls to see how the chat format works in practice.</p>"},{"location":"examples/inference-endpoints-example/","title":"Hugging Face Inference Endpoints Example","text":"In\u00a0[\u00a0]: Copied! <pre># if needed, install and/or upgrade to the latest version of the OpenAI Python library\n%pip install --upgrade easyllm\n</pre> # if needed, install and/or upgrade to the latest version of the OpenAI Python library %pip install --upgrade easyllm  In\u00a0[1]: Copied! <pre>from easyllm.clients import huggingface\nfrom easyllm.prompt_utils import llama2_stop_sequences, build_llama2_prompt\n\nhuggingface.prompt_builder = build_llama2_prompt\n\n# Here you can overwrite the url to your endpoint, can also be localhost:8000\nhuggingface.api_base = \"YOUR_ENDPOINT_URL\"\n\n# The module automatically loads the HuggingFace API key from the environment variable HUGGINGFACE_TOKEN or from the HuggingFace CLI configuration file.\n# huggingface.api_key=\"hf_xxx\"\n\nresponse = huggingface.ChatCompletion.create(\n    messages=[\n        {\"role\": \"system\", \"content\": \"\\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\"},\n        {\"role\": \"user\", \"content\": \"Knock knock.\"},\n        {\"role\": \"assistant\", \"content\": \"Who's there?\"},\n        {\"role\": \"user\", \"content\": \"Apple.\"},\n    ],\n      temperature=0.9,\n      top_p=0.6,\n      max_tokens=1024,\n)\nresponse\n</pre> from easyllm.clients import huggingface from easyllm.prompt_utils import llama2_stop_sequences, build_llama2_prompt  huggingface.prompt_builder = build_llama2_prompt  # Here you can overwrite the url to your endpoint, can also be localhost:8000 huggingface.api_base = \"YOUR_ENDPOINT_URL\"  # The module automatically loads the HuggingFace API key from the environment variable HUGGINGFACE_TOKEN or from the HuggingFace CLI configuration file. # huggingface.api_key=\"hf_xxx\"  response = huggingface.ChatCompletion.create(     messages=[         {\"role\": \"system\", \"content\": \"\\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\"},         {\"role\": \"user\", \"content\": \"Knock knock.\"},         {\"role\": \"assistant\", \"content\": \"Who's there?\"},         {\"role\": \"user\", \"content\": \"Apple.\"},     ],       temperature=0.9,       top_p=0.6,       max_tokens=1024, ) response Out[1]: <pre>{'id': 'hf-0lL5H_yyRR',\n 'object': 'chat.completion',\n 'created': 1691096023,\n 'choices': [{'index': 0,\n   'message': {'role': 'assistant', 'content': ' Apple who?'},\n   'finish_reason': 'eos_token'}],\n 'usage': {'prompt_tokens': 149, 'completion_tokens': 5, 'total_tokens': 154}}</pre> <p>As you can see, the response object has a few fields:</p> <ul> <li><code>id</code>: the ID of the request</li> <li><code>object</code>: the type of object returned (e.g., <code>chat.completion</code>)</li> <li><code>created</code>: the timestamp of the request</li> <li><code>model</code>: the full name of the model used to generate the response</li> <li><code>usage</code>: the number of tokens used to generate the replies, counting prompt, completion, and total</li> <li><code>choices</code>: a list of completion objects (only one, unless you set <code>n</code> greater than 1)<ul> <li><code>message</code>: the message object generated by the model, with <code>role</code> and <code>content</code></li> <li><code>finish_reason</code>: the reason the model stopped generating text (either <code>stop</code>, or <code>length</code> if <code>max_tokens</code> limit was reached)</li> <li><code>index</code>: the index of the completion in the list of choices</li> </ul> </li> </ul> <p>Extract just the reply with:</p> In\u00a0[2]: Copied! <pre>print(response['choices'][0]['message']['content'])\n</pre> print(response['choices'][0]['message']['content']) <pre> Apple who?\n</pre> In\u00a0[6]: Copied! <pre>from easyllm.clients import huggingface\nfrom easyllm.prompt_utils import llama2_stop_sequences, build_llama2_prompt\n\nhuggingface.prompt_builder = build_llama2_prompt\n\n# Here you can overwrite the url to your endpoint, can also be localhost:8000\nhuggingface.api_base = \"YOUR_ENDPOINT_URL\"\n\n# a ChatCompletion request\nresponse = huggingface.ChatCompletion.create(\n    messages=[\n        {'role': 'user', 'content': \"Count to 10.\"}\n    ],\n    stream=True  # this time, we set stream=True\n)\n\nfor chunk in response:\n    delta = chunk['choices'][0]['delta']\n    if \"content\" in delta:\n        print(delta[\"content\"],end=\"\")\n</pre> from easyllm.clients import huggingface from easyllm.prompt_utils import llama2_stop_sequences, build_llama2_prompt  huggingface.prompt_builder = build_llama2_prompt  # Here you can overwrite the url to your endpoint, can also be localhost:8000 huggingface.api_base = \"YOUR_ENDPOINT_URL\"  # a ChatCompletion request response = huggingface.ChatCompletion.create(     messages=[         {'role': 'user', 'content': \"Count to 10.\"}     ],     stream=True  # this time, we set stream=True )  for chunk in response:     delta = chunk['choices'][0]['delta']     if \"content\" in delta:         print(delta[\"content\"],end=\"\") <pre>  Sure! Here we go:\n\n1. One\n2. Two\n3. Three\n4. Four\n5. Five\n6. Six\n7. Seven\n8. Eight\n9. Nine\n10. Ten!</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"examples/inference-endpoints-example/#hugging-face-inference-endpoints-example","title":"Hugging Face Inference Endpoints Example\u00b6","text":"<p>Hugging Face Inference Endpoints\u00a0offers an easy and secure way to deploy Machine Learning models for use in production. Inference Endpoints empower developers and data scientists alike to create AI applications without managing infrastructure: simplifying the deployment process to a few clicks, including handling large volumes of requests with autoscaling, reducing infrastructure costs with scale-to-zero, and offering advanced security.</p> <p>You can get started with Inference Endpoints at:\u00a0https://ui.endpoints.huggingface.co/</p> <p>The example assumes that you have an running endpoint for a conversational model, e.g. <code>https://huggingface.co/meta-llama/Llama-2-13b-chat-hf</code></p>"},{"location":"examples/inference-endpoints-example/#1-import-the-easyllm-library","title":"1. Import the easyllm library\u00b6","text":""},{"location":"examples/inference-endpoints-example/#2-an-example-chat-api-call","title":"2. An example chat API call\u00b6","text":"<p>Since we want to use our endpoint for inference we don't have to define the <code>model</code> parameter. We either need to expose an environment variable <code>HUGGINGFACE_API_BASE</code> before the import of <code>easyllm.clients.huggingface</code> or overwrite the <code>huggingface.api_base</code> value.</p> <p>A chat API call then only has two required inputs:</p> <ul> <li><code>messages</code>: a list of message objects, where each object has two required fields:<ul> <li><code>role</code>: the role of the messenger (either <code>system</code>, <code>user</code>, or <code>assistant</code>)</li> <li><code>content</code>: the content of the message (e.g., <code>Write me a beautiful poem</code>)</li> </ul> </li> </ul> <p>Let's look at an example chat API calls to see how the chat format works in practice.</p>"},{"location":"examples/inference-endpoints-example/#how-to-stream-chat-completion-requests","title":"How to stream Chat Completion requests\u00b6","text":"<p>Custom endpoints can be created to stream chat completion requests to a model.</p>"},{"location":"examples/stream-chat-completions/","title":"How to stream Chat Completion requests","text":"In\u00a0[1]: Copied! <pre># imports\nimport easyllm  # for API calls\nimport time  # for measuring time duration of API calls\n</pre> # imports import easyllm  # for API calls import time  # for measuring time duration of API calls In\u00a0[3]: Copied! <pre>from easyllm.clients import huggingface\nfrom easyllm.prompt_utils import build_llama2_prompt\nhuggingface.prompt_builder = build_llama2_prompt\n\n# record the time before the request is sent\nstart_time = time.time()\n\n# send a ChatCompletion request to count to 100\nresponse = huggingface.ChatCompletion.create(\n    model=\"meta-llama/Llama-2-70b-chat-hf\",\n    messages=[\n        {'role': 'user', 'content': 'Count to 100, with a comma between each number and no newlines. E.g., 1, 2, 3, ...'}\n    ],\n)\n\n# calculate the time it took to receive the response\nresponse_time = time.time() - start_time\n\n# print the time delay and text received\nprint(f\"Full response received {response_time:.2f} seconds after request\")\nprint(f\"Full response received:\\n{response}\")\n</pre> from easyllm.clients import huggingface from easyllm.prompt_utils import build_llama2_prompt huggingface.prompt_builder = build_llama2_prompt  # record the time before the request is sent start_time = time.time()  # send a ChatCompletion request to count to 100 response = huggingface.ChatCompletion.create(     model=\"meta-llama/Llama-2-70b-chat-hf\",     messages=[         {'role': 'user', 'content': 'Count to 100, with a comma between each number and no newlines. E.g., 1, 2, 3, ...'}     ], )  # calculate the time it took to receive the response response_time = time.time() - start_time  # print the time delay and text received print(f\"Full response received {response_time:.2f} seconds after request\") print(f\"Full response received:\\n{response}\")  <pre>Full response received 16.31 seconds after request\nFull response received:\n{'id': 'hf-r90jgIFTUC', 'object': 'chat.completion', 'created': 1690658234, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': ' Sure! Here it is:\\n\\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100'}, 'finish_reason': None}], 'usage': {'prompt_tokens': 100, 'completion_tokens': 410, 'total_tokens': 510}}\n</pre> <p>The reply can be extracted with <code>response['choices'][0]['message']</code>.</p> <p>The content of the reply can be extracted with <code>response['choices'][0]['message']['content']</code>.</p> In\u00a0[4]: Copied! <pre>reply = response['choices'][0]['message']\nprint(f\"Extracted reply: \\n{reply}\")\n\nreply_content = response['choices'][0]['message']['content']\nprint(f\"Extracted content: \\n{reply_content}\")\n</pre> reply = response['choices'][0]['message'] print(f\"Extracted reply: \\n{reply}\")  reply_content = response['choices'][0]['message']['content'] print(f\"Extracted content: \\n{reply_content}\")  <pre>Extracted reply: \n{'role': 'assistant', 'content': ' Sure! Here it is:\\n\\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100'}\nExtracted content: \n Sure! Here it is:\n\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100\n</pre> In\u00a0[2]: Copied! <pre>from easyllm.clients import huggingface\nfrom easyllm.prompt_utils import build_llama2_prompt\nhuggingface.prompt_builder = build_llama2_prompt\n\n# a ChatCompletion request\nresponse = huggingface.ChatCompletion.create(\n    model=\"meta-llama/Llama-2-70b-chat-hf\",\n    messages=[\n        {'role': 'user', 'content': \"What's 1+1? Answer in one word.\"}\n    ],\n    stream=True  # this time, we set stream=True\n)\n\nfor chunk in response:\n    print(chunk)\n</pre> from easyllm.clients import huggingface from easyllm.prompt_utils import build_llama2_prompt huggingface.prompt_builder = build_llama2_prompt  # a ChatCompletion request response = huggingface.ChatCompletion.create(     model=\"meta-llama/Llama-2-70b-chat-hf\",     messages=[         {'role': 'user', 'content': \"What's 1+1? Answer in one word.\"}     ],     stream=True  # this time, we set stream=True )  for chunk in response:     print(chunk) <pre>{'id': 'hf-ijGQiCc2bQ', 'object': 'chat.completion.chunk', 'created': 1690659356, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'delta': {'role': 'assistant', 'content': None}, 'finish_reason': None}]}\n{'id': 'hf-ijGQiCc2bQ', 'object': 'chat.completion.chunk', 'created': 1690659356, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'delta': {'role': None, 'content': ' '}, 'finish_reason': None}]}\n{'id': 'hf-ijGQiCc2bQ', 'object': 'chat.completion.chunk', 'created': 1690659356, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'delta': {'role': None, 'content': ' Sure'}, 'finish_reason': None}]}\n{'id': 'hf-ijGQiCc2bQ', 'object': 'chat.completion.chunk', 'created': 1690659356, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'delta': {'role': None, 'content': '!'}, 'finish_reason': None}]}\n{'id': 'hf-ijGQiCc2bQ', 'object': 'chat.completion.chunk', 'created': 1690659357, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'delta': {'role': None, 'content': ' Here'}, 'finish_reason': None}]}\n{'id': 'hf-ijGQiCc2bQ', 'object': 'chat.completion.chunk', 'created': 1690659357, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'delta': {'role': None, 'content': \"'\"}, 'finish_reason': None}]}\n{'id': 'hf-ijGQiCc2bQ', 'object': 'chat.completion.chunk', 'created': 1690659357, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'delta': {'role': None, 'content': 's'}, 'finish_reason': None}]}\n{'id': 'hf-ijGQiCc2bQ', 'object': 'chat.completion.chunk', 'created': 1690659357, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'delta': {'role': None, 'content': ' a'}, 'finish_reason': None}]}\n{'id': 'hf-ijGQiCc2bQ', 'object': 'chat.completion.chunk', 'created': 1690659357, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'delta': {'role': None, 'content': ' step'}, 'finish_reason': None}]}\n{'id': 'hf-ijGQiCc2bQ', 'object': 'chat.completion.chunk', 'created': 1690659357, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'delta': {'role': None, 'content': '-'}, 'finish_reason': None}]}\n{'id': 'hf-ijGQiCc2bQ', 'object': 'chat.completion.chunk', 'created': 1690659357, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'delta': {'role': None, 'content': 'by'}, 'finish_reason': None}]}\n{'id': 'hf-ijGQiCc2bQ', 'object': 'chat.completion.chunk', 'created': 1690659357, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'delta': {'role': None, 'content': '-'}, 'finish_reason': None}]}\n{'id': 'hf-ijGQiCc2bQ', 'object': 'chat.completion.chunk', 'created': 1690659357, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'delta': {'role': None, 'content': 'step'}, 'finish_reason': None}]}\n{'id': 'hf-ijGQiCc2bQ', 'object': 'chat.completion.chunk', 'created': 1690659357, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'delta': {'role': None, 'content': ' explanation'}, 'finish_reason': None}]}\n{'id': 'hf-ijGQiCc2bQ', 'object': 'chat.completion.chunk', 'created': 1690659357, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'delta': {'role': None, 'content': ' of'}, 'finish_reason': None}]}\n{'id': 'hf-ijGQiCc2bQ', 'object': 'chat.completion.chunk', 'created': 1690659357, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'delta': {'role': None, 'content': ' how'}, 'finish_reason': None}]}\n{'id': 'hf-ijGQiCc2bQ', 'object': 'chat.completion.chunk', 'created': 1690659357, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'delta': {'role': None, 'content': ' to'}, 'finish_reason': None}]}\n{'id': 'hf-ijGQiCc2bQ', 'object': 'chat.completion.chunk', 'created': 1690659357, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'delta': {'role': None, 'content': ' solve'}, 'finish_reason': None}]}\n{'id': 'hf-ijGQiCc2bQ', 'object': 'chat.completion.chunk', 'created': 1690659357, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'delta': {'role': None, 'content': ' the'}, 'finish_reason': None}]}\n{'id': 'hf-ijGQiCc2bQ', 'object': 'chat.completion.chunk', 'created': 1690659357, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'delta': {'role': None, 'content': ' problem'}, 'finish_reason': None}]}\n{'id': 'hf-ijGQiCc2bQ', 'object': 'chat.completion.chunk', 'created': 1690659357, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'delta': {'role': None, 'content': ' '}, 'finish_reason': None}]}\n{'id': 'hf-ijGQiCc2bQ', 'object': 'chat.completion.chunk', 'created': 1690659357, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'delta': {'role': None, 'content': '1'}, 'finish_reason': None}]}\n{'id': 'hf-ijGQiCc2bQ', 'object': 'chat.completion.chunk', 'created': 1690659357, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'delta': {'role': None, 'content': '+'}, 'finish_reason': None}]}\n{'id': 'hf-ijGQiCc2bQ', 'object': 'chat.completion.chunk', 'created': 1690659357, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'delta': {'role': None, 'content': '1'}, 'finish_reason': None}]}\n{'id': 'hf-ijGQiCc2bQ', 'object': 'chat.completion.chunk', 'created': 1690659357, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'delta': {'role': None, 'content': ':'}, 'finish_reason': None}]}\n{'id': 'hf-ijGQiCc2bQ', 'object': 'chat.completion.chunk', 'created': 1690659357, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'delta': {'role': None, 'content': '\\n'}, 'finish_reason': None}]}\n{'id': 'hf-ijGQiCc2bQ', 'object': 'chat.completion.chunk', 'created': 1690659357, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'delta': {'role': None, 'content': '\\n'}, 'finish_reason': None}]}\n{'id': 'hf-ijGQiCc2bQ', 'object': 'chat.completion.chunk', 'created': 1690659357, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'delta': {'role': None, 'content': '1'}, 'finish_reason': None}]}\n{'id': 'hf-ijGQiCc2bQ', 'object': 'chat.completion.chunk', 'created': 1690659358, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'delta': {'role': None, 'content': '.'}, 'finish_reason': None}]}\n{'id': 'hf-ijGQiCc2bQ', 'object': 'chat.completion.chunk', 'created': 1690659358, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'delta': {'role': None, 'content': ' Start'}, 'finish_reason': None}]}\n{'id': 'hf-ijGQiCc2bQ', 'object': 'chat.completion.chunk', 'created': 1690659358, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'delta': {'role': None, 'content': ' with'}, 'finish_reason': None}]}\n{'id': 'hf-ijGQiCc2bQ', 'object': 'chat.completion.chunk', 'created': 1690659358, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'delta': {'role': None, 'content': ' the'}, 'finish_reason': None}]}\n{'id': 'hf-ijGQiCc2bQ', 'object': 'chat.completion.chunk', 'created': 1690659358, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'delta': {'role': None, 'content': ' first'}, 'finish_reason': None}]}\n{'id': 'hf-ijGQiCc2bQ', 'object': 'chat.completion.chunk', 'created': 1690659358, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'delta': {'role': None, 'content': ' number'}, 'finish_reason': None}]}\n{'id': 'hf-ijGQiCc2bQ', 'object': 'chat.completion.chunk', 'created': 1690659358, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'delta': {'role': None, 'content': ','}, 'finish_reason': None}]}\n{'id': 'hf-ijGQiCc2bQ', 'object': 'chat.completion.chunk', 'created': 1690659358, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'delta': {'role': None, 'content': ' '}, 'finish_reason': None}]}\n{'id': 'hf-ijGQiCc2bQ', 'object': 'chat.completion.chunk', 'created': 1690659358, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'delta': {'role': None, 'content': '1'}, 'finish_reason': None}]}\n{'id': 'hf-ijGQiCc2bQ', 'object': 'chat.completion.chunk', 'created': 1690659358, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'delta': {'role': None, 'content': '.'}, 'finish_reason': None}]}\n{'id': 'hf-ijGQiCc2bQ', 'object': 'chat.completion.chunk', 'created': 1690659358, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'delta': {'role': None, 'content': '\\n'}, 'finish_reason': None}]}\n{'id': 'hf-ijGQiCc2bQ', 'object': 'chat.completion.chunk', 'created': 1690659358, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'delta': {'role': None, 'content': '2'}, 'finish_reason': None}]}\n{'id': 'hf-ijGQiCc2bQ', 'object': 'chat.completion.chunk', 'created': 1690659358, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'delta': {'role': None, 'content': '.'}, 'finish_reason': None}]}\n{'id': 'hf-ijGQiCc2bQ', 'object': 'chat.completion.chunk', 'created': 1690659358, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'delta': {'role': None, 'content': ' Start'}, 'finish_reason': None}]}\n{'id': 'hf-ijGQiCc2bQ', 'object': 'chat.completion.chunk', 'created': 1690659358, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'delta': {'role': None, 'content': ' with'}, 'finish_reason': None}]}\n{'id': 'hf-ijGQiCc2bQ', 'object': 'chat.completion.chunk', 'created': 1690659358, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'delta': {'role': None, 'content': ' the'}, 'finish_reason': None}]}\n{'id': 'hf-ijGQiCc2bQ', 'object': 'chat.completion.chunk', 'created': 1690659358, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'delta': {'role': None, 'content': ' second'}, 'finish_reason': None}]}\n{'id': 'hf-ijGQiCc2bQ', 'object': 'chat.completion.chunk', 'created': 1690659358, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'delta': {'role': None, 'content': ' number'}, 'finish_reason': None}]}\n{'id': 'hf-ijGQiCc2bQ', 'object': 'chat.completion.chunk', 'created': 1690659358, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'delta': {'role': None, 'content': ','}, 'finish_reason': None}]}\n{'id': 'hf-ijGQiCc2bQ', 'object': 'chat.completion.chunk', 'created': 1690659358, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'delta': {'role': None, 'content': ' '}, 'finish_reason': None}]}\n{'id': 'hf-ijGQiCc2bQ', 'object': 'chat.completion.chunk', 'created': 1690659358, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'delta': {'role': None, 'content': '1'}, 'finish_reason': None}]}\n{'id': 'hf-ijGQiCc2bQ', 'object': 'chat.completion.chunk', 'created': 1690659358, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'delta': {'role': None, 'content': '.'}, 'finish_reason': None}]}\n{'id': 'hf-ijGQiCc2bQ', 'object': 'chat.completion.chunk', 'created': 1690659358, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'delta': {'role': None, 'content': '\\n'}, 'finish_reason': None}]}\n{'id': 'hf-ijGQiCc2bQ', 'object': 'chat.completion.chunk', 'created': 1690659358, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'delta': {'role': None, 'content': '3'}, 'finish_reason': None}]}\n{'id': 'hf-ijGQiCc2bQ', 'object': 'chat.completion.chunk', 'created': 1690659359, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'delta': {'role': None, 'content': '.'}, 'finish_reason': None}]}\n{'id': 'hf-ijGQiCc2bQ', 'object': 'chat.completion.chunk', 'created': 1690659359, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'delta': {'role': None, 'content': ' Add'}, 'finish_reason': None}]}\n{'id': 'hf-ijGQiCc2bQ', 'object': 'chat.completion.chunk', 'created': 1690659359, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'delta': {'role': None, 'content': ' the'}, 'finish_reason': None}]}\n{'id': 'hf-ijGQiCc2bQ', 'object': 'chat.completion.chunk', 'created': 1690659359, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'delta': {'role': None, 'content': ' two'}, 'finish_reason': None}]}\n{'id': 'hf-ijGQiCc2bQ', 'object': 'chat.completion.chunk', 'created': 1690659359, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'delta': {'role': None, 'content': ' numbers'}, 'finish_reason': None}]}\n{'id': 'hf-ijGQiCc2bQ', 'object': 'chat.completion.chunk', 'created': 1690659359, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'delta': {'role': None, 'content': ' together'}, 'finish_reason': None}]}\n{'id': 'hf-ijGQiCc2bQ', 'object': 'chat.completion.chunk', 'created': 1690659359, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'delta': {'role': None, 'content': '.'}, 'finish_reason': None}]}\n{'id': 'hf-ijGQiCc2bQ', 'object': 'chat.completion.chunk', 'created': 1690659359, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'delta': {'role': None, 'content': ' In'}, 'finish_reason': None}]}\n{'id': 'hf-ijGQiCc2bQ', 'object': 'chat.completion.chunk', 'created': 1690659359, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'delta': {'role': None, 'content': ' this'}, 'finish_reason': None}]}\n{'id': 'hf-ijGQiCc2bQ', 'object': 'chat.completion.chunk', 'created': 1690659359, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'delta': {'role': None, 'content': ' case'}, 'finish_reason': None}]}\n{'id': 'hf-ijGQiCc2bQ', 'object': 'chat.completion.chunk', 'created': 1690659359, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'delta': {'role': None, 'content': ','}, 'finish_reason': None}]}\n{'id': 'hf-ijGQiCc2bQ', 'object': 'chat.completion.chunk', 'created': 1690659359, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'delta': {'role': None, 'content': ' '}, 'finish_reason': None}]}\n{'id': 'hf-ijGQiCc2bQ', 'object': 'chat.completion.chunk', 'created': 1690659359, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'delta': {'role': None, 'content': '1'}, 'finish_reason': None}]}\n{'id': 'hf-ijGQiCc2bQ', 'object': 'chat.completion.chunk', 'created': 1690659359, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'delta': {'role': None, 'content': ' +'}, 'finish_reason': None}]}\n{'id': 'hf-ijGQiCc2bQ', 'object': 'chat.completion.chunk', 'created': 1690659359, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'delta': {'role': None, 'content': ' '}, 'finish_reason': None}]}\n{'id': 'hf-ijGQiCc2bQ', 'object': 'chat.completion.chunk', 'created': 1690659359, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'delta': {'role': None, 'content': '1'}, 'finish_reason': None}]}\n{'id': 'hf-ijGQiCc2bQ', 'object': 'chat.completion.chunk', 'created': 1690659359, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'delta': {'role': None, 'content': ' ='}, 'finish_reason': None}]}\n{'id': 'hf-ijGQiCc2bQ', 'object': 'chat.completion.chunk', 'created': 1690659359, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'delta': {'role': None, 'content': ' '}, 'finish_reason': None}]}\n{'id': 'hf-ijGQiCc2bQ', 'object': 'chat.completion.chunk', 'created': 1690659359, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'delta': {'role': None, 'content': '2'}, 'finish_reason': None}]}\n{'id': 'hf-ijGQiCc2bQ', 'object': 'chat.completion.chunk', 'created': 1690659359, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'delta': {'role': None, 'content': '.'}, 'finish_reason': None}]}\n{'id': 'hf-ijGQiCc2bQ', 'object': 'chat.completion.chunk', 'created': 1690659359, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'delta': {'role': None, 'content': '\\n'}, 'finish_reason': None}]}\n{'id': 'hf-ijGQiCc2bQ', 'object': 'chat.completion.chunk', 'created': 1690659359, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'delta': {'role': None, 'content': '4'}, 'finish_reason': None}]}\n{'id': 'hf-ijGQiCc2bQ', 'object': 'chat.completion.chunk', 'created': 1690659359, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'delta': {'role': None, 'content': '.'}, 'finish_reason': None}]}\n{'id': 'hf-ijGQiCc2bQ', 'object': 'chat.completion.chunk', 'created': 1690659359, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'delta': {'role': None, 'content': ' The'}, 'finish_reason': None}]}\n{'id': 'hf-ijGQiCc2bQ', 'object': 'chat.completion.chunk', 'created': 1690659359, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'delta': {'role': None, 'content': ' result'}, 'finish_reason': None}]}\n{'id': 'hf-ijGQiCc2bQ', 'object': 'chat.completion.chunk', 'created': 1690659360, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'delta': {'role': None, 'content': ' of'}, 'finish_reason': None}]}\n{'id': 'hf-ijGQiCc2bQ', 'object': 'chat.completion.chunk', 'created': 1690659360, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'delta': {'role': None, 'content': ' the'}, 'finish_reason': None}]}\n{'id': 'hf-ijGQiCc2bQ', 'object': 'chat.completion.chunk', 'created': 1690659360, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'delta': {'role': None, 'content': ' addition'}, 'finish_reason': None}]}\n{'id': 'hf-ijGQiCc2bQ', 'object': 'chat.completion.chunk', 'created': 1690659360, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'delta': {'role': None, 'content': ' is'}, 'finish_reason': None}]}\n{'id': 'hf-ijGQiCc2bQ', 'object': 'chat.completion.chunk', 'created': 1690659360, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'delta': {'role': None, 'content': ' '}, 'finish_reason': None}]}\n{'id': 'hf-ijGQiCc2bQ', 'object': 'chat.completion.chunk', 'created': 1690659360, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'delta': {'role': None, 'content': '2'}, 'finish_reason': None}]}\n{'id': 'hf-ijGQiCc2bQ', 'object': 'chat.completion.chunk', 'created': 1690659360, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'delta': {'role': None, 'content': ','}, 'finish_reason': None}]}\n{'id': 'hf-ijGQiCc2bQ', 'object': 'chat.completion.chunk', 'created': 1690659360, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'delta': {'role': None, 'content': ' so'}, 'finish_reason': None}]}\n{'id': 'hf-ijGQiCc2bQ', 'object': 'chat.completion.chunk', 'created': 1690659360, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'delta': {'role': None, 'content': ' that'}, 'finish_reason': None}]}\n{'id': 'hf-ijGQiCc2bQ', 'object': 'chat.completion.chunk', 'created': 1690659360, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'delta': {'role': None, 'content': ' is'}, 'finish_reason': None}]}\n{'id': 'hf-ijGQiCc2bQ', 'object': 'chat.completion.chunk', 'created': 1690659360, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'delta': {'role': None, 'content': ' the'}, 'finish_reason': None}]}\n{'id': 'hf-ijGQiCc2bQ', 'object': 'chat.completion.chunk', 'created': 1690659360, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'delta': {'role': None, 'content': ' final'}, 'finish_reason': None}]}\n{'id': 'hf-ijGQiCc2bQ', 'object': 'chat.completion.chunk', 'created': 1690659360, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'delta': {'role': None, 'content': ' answer'}, 'finish_reason': None}]}\n{'id': 'hf-ijGQiCc2bQ', 'object': 'chat.completion.chunk', 'created': 1690659360, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'delta': {'role': None, 'content': '.'}, 'finish_reason': None}]}\n{'id': 'hf-ijGQiCc2bQ', 'object': 'chat.completion.chunk', 'created': 1690659360, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'delta': {'role': None, 'content': '\\n'}, 'finish_reason': None}]}\n{'id': 'hf-ijGQiCc2bQ', 'object': 'chat.completion.chunk', 'created': 1690659360, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'delta': {'role': None, 'content': '\\n'}, 'finish_reason': None}]}\n{'id': 'hf-ijGQiCc2bQ', 'object': 'chat.completion.chunk', 'created': 1690659360, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'delta': {'role': None, 'content': 'There'}, 'finish_reason': None}]}\n{'id': 'hf-ijGQiCc2bQ', 'object': 'chat.completion.chunk', 'created': 1690659360, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'delta': {'role': None, 'content': 'fore'}, 'finish_reason': None}]}\n{'id': 'hf-ijGQiCc2bQ', 'object': 'chat.completion.chunk', 'created': 1690659360, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'delta': {'role': None, 'content': ','}, 'finish_reason': None}]}\n{'id': 'hf-ijGQiCc2bQ', 'object': 'chat.completion.chunk', 'created': 1690659360, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'delta': {'role': None, 'content': ' '}, 'finish_reason': None}]}\n{'id': 'hf-ijGQiCc2bQ', 'object': 'chat.completion.chunk', 'created': 1690659360, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'delta': {'role': None, 'content': '1'}, 'finish_reason': None}]}\n{'id': 'hf-ijGQiCc2bQ', 'object': 'chat.completion.chunk', 'created': 1690659360, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'delta': {'role': None, 'content': '+'}, 'finish_reason': None}]}\n{'id': 'hf-ijGQiCc2bQ', 'object': 'chat.completion.chunk', 'created': 1690659360, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'delta': {'role': None, 'content': '1'}, 'finish_reason': None}]}\n{'id': 'hf-ijGQiCc2bQ', 'object': 'chat.completion.chunk', 'created': 1690659360, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'delta': {'role': None, 'content': ' ='}, 'finish_reason': None}]}\n{'id': 'hf-ijGQiCc2bQ', 'object': 'chat.completion.chunk', 'created': 1690659360, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'delta': {'role': None, 'content': ' '}, 'finish_reason': None}]}\n{'id': 'hf-ijGQiCc2bQ', 'object': 'chat.completion.chunk', 'created': 1690659360, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'delta': {'role': None, 'content': '2'}, 'finish_reason': None}]}\n{'id': 'hf-ijGQiCc2bQ', 'object': 'chat.completion.chunk', 'created': 1690659360, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'delta': {'role': None, 'content': '.'}, 'finish_reason': None}]}\n{'id': 'hf-ijGQiCc2bQ', 'object': 'chat.completion.chunk', 'created': 1690659361, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'delta': {'role': None, 'content': None}, 'finish_reason': None}]}\n</pre> <p>As you can see above, streaming responses have a <code>delta</code> field rather than a <code>message</code> field. <code>delta</code> can hold things like:</p> <ul> <li>a role token (e.g., <code>{\"role\": \"assistant\"}</code>)</li> <li>a content token (e.g., <code>{\"content\": \"\\n\\n\"}</code>)</li> <li>nothing (e.g., <code>{}</code>), when the stream is over</li> </ul> In\u00a0[2]: Copied! <pre>import time\nfrom easyllm.clients import huggingface\nfrom easyllm.prompt_utils import build_llama2_prompt\nhuggingface.prompt_builder = build_llama2_prompt\n\n# record the time before the request is sent\nstart_time = time.time()\n\n# send a ChatCompletion request to count to 100\nresponse = huggingface.ChatCompletion.create(\n    model=\"meta-llama/Llama-2-70b-chat-hf\",\n    messages=[\n        {'role': 'user', 'content': 'Count to 50, with a comma between each number and no newlines. E.g., 1, 2, 3, ...'}\n    ],\n    stream=True  # again, we set stream=True\n)\n\n# create variables to collect the stream of chunks\ncollected_chunks = []\ncollected_messages = []\n# iterate through the stream of events\nfor chunk in response:\n    chunk_time = time.time() - start_time  # calculate the time delay of the chunk\n    collected_chunks.append(chunk)  # save the event response\n    chunk_message = chunk['choices'][0]['delta']  # extract the message\n    collected_messages.append(chunk_message)  # save the message\n    print(f\"Message received {chunk_time:.2f} seconds after request: {chunk_message}\")  # print the delay and text\n\n# print the time delay and text received\nprint(f\"Full response received {chunk_time:.2f} seconds after request\")\nfull_reply_content = ''.join([m.get('content', '') for m in collected_messages])\nprint(f\"Full conversation received: {full_reply_content}\")\n</pre> import time from easyllm.clients import huggingface from easyllm.prompt_utils import build_llama2_prompt huggingface.prompt_builder = build_llama2_prompt  # record the time before the request is sent start_time = time.time()  # send a ChatCompletion request to count to 100 response = huggingface.ChatCompletion.create(     model=\"meta-llama/Llama-2-70b-chat-hf\",     messages=[         {'role': 'user', 'content': 'Count to 50, with a comma between each number and no newlines. E.g., 1, 2, 3, ...'}     ],     stream=True  # again, we set stream=True )  # create variables to collect the stream of chunks collected_chunks = [] collected_messages = [] # iterate through the stream of events for chunk in response:     chunk_time = time.time() - start_time  # calculate the time delay of the chunk     collected_chunks.append(chunk)  # save the event response     chunk_message = chunk['choices'][0]['delta']  # extract the message     collected_messages.append(chunk_message)  # save the message     print(f\"Message received {chunk_time:.2f} seconds after request: {chunk_message}\")  # print the delay and text  # print the time delay and text received print(f\"Full response received {chunk_time:.2f} seconds after request\") full_reply_content = ''.join([m.get('content', '') for m in collected_messages]) print(f\"Full conversation received: {full_reply_content}\")  <pre>Message received 0.31 seconds after request: {'role': 'assistant'}\nMessage received 0.36 seconds after request: {'content': ' '}\nMessage received 0.36 seconds after request: {'content': ' Sure'}\nMessage received 0.41 seconds after request: {'content': '!'}\nMessage received 0.46 seconds after request: {'content': ' Here'}\nMessage received 0.51 seconds after request: {'content': ' it'}\nMessage received 0.56 seconds after request: {'content': ' is'}\nMessage received 0.61 seconds after request: {'content': ':'}\nMessage received 0.66 seconds after request: {'content': '\\n'}\nMessage received 0.72 seconds after request: {'content': '\\n'}\nMessage received 0.72 seconds after request: {'content': '1'}\nMessage received 0.77 seconds after request: {'content': ','}\nMessage received 0.82 seconds after request: {'content': ' '}\nMessage received 0.87 seconds after request: {'content': '2'}\nMessage received 0.92 seconds after request: {'content': ','}\nMessage received 0.97 seconds after request: {'content': ' '}\nMessage received 1.02 seconds after request: {'content': '3'}\nMessage received 1.07 seconds after request: {'content': ','}\nMessage received 1.12 seconds after request: {'content': ' '}\nMessage received 1.12 seconds after request: {'content': '4'}\nMessage received 1.17 seconds after request: {'content': ','}\nMessage received 1.23 seconds after request: {'content': ' '}\nMessage received 1.28 seconds after request: {'content': '5'}\nMessage received 1.33 seconds after request: {'content': ','}\nMessage received 1.39 seconds after request: {'content': ' '}\nMessage received 1.44 seconds after request: {'content': '6'}\nMessage received 1.49 seconds after request: {'content': ','}\nMessage received 1.54 seconds after request: {'content': ' '}\nMessage received 1.54 seconds after request: {'content': '7'}\nMessage received 1.59 seconds after request: {'content': ','}\nMessage received 1.64 seconds after request: {'content': ' '}\nMessage received 1.69 seconds after request: {'content': '8'}\nMessage received 1.75 seconds after request: {'content': ','}\nMessage received 1.79 seconds after request: {'content': ' '}\nMessage received 1.85 seconds after request: {'content': '9'}\nMessage received 1.90 seconds after request: {'content': ','}\nMessage received 1.95 seconds after request: {'content': ' '}\nMessage received 2.00 seconds after request: {'content': '1'}\nMessage received 2.05 seconds after request: {'content': '0'}\nMessage received 2.05 seconds after request: {'content': ','}\nMessage received 2.10 seconds after request: {'content': ' '}\nMessage received 2.15 seconds after request: {'content': '1'}\nMessage received 2.20 seconds after request: {'content': '1'}\nMessage received 2.26 seconds after request: {'content': ','}\nMessage received 2.31 seconds after request: {'content': ' '}\nMessage received 2.36 seconds after request: {'content': '1'}\nMessage received 2.42 seconds after request: {'content': '2'}\nMessage received 2.42 seconds after request: {'content': ','}\nMessage received 2.46 seconds after request: {'content': ' '}\nMessage received 2.51 seconds after request: {'content': '1'}\nMessage received 2.56 seconds after request: {'content': '3'}\nMessage received 2.61 seconds after request: {'content': ','}\nMessage received 2.66 seconds after request: {'content': ' '}\nMessage received 2.72 seconds after request: {'content': '1'}\nMessage received 2.77 seconds after request: {'content': '4'}\nMessage received 2.82 seconds after request: {'content': ','}\nMessage received 2.87 seconds after request: {'content': ' '}\nMessage received 2.87 seconds after request: {'content': '1'}\nMessage received 2.92 seconds after request: {'content': '5'}\nMessage received 2.97 seconds after request: {'content': ','}\nMessage received 3.03 seconds after request: {'content': ' '}\nMessage received 3.07 seconds after request: {'content': '1'}\nMessage received 3.13 seconds after request: {'content': '6'}\nMessage received 3.17 seconds after request: {'content': ','}\nMessage received 3.23 seconds after request: {'content': ' '}\nMessage received 3.28 seconds after request: {'content': '1'}\nMessage received 3.33 seconds after request: {'content': '7'}\nMessage received 3.33 seconds after request: {'content': ','}\nMessage received 3.38 seconds after request: {'content': ' '}\nMessage received 3.43 seconds after request: {'content': '1'}\nMessage received 3.48 seconds after request: {'content': '8'}\nMessage received 3.53 seconds after request: {'content': ','}\nMessage received 3.59 seconds after request: {'content': ' '}\nMessage received 3.65 seconds after request: {'content': '1'}\nMessage received 3.71 seconds after request: {'content': '9'}\nMessage received 3.71 seconds after request: {'content': ','}\nMessage received 3.76 seconds after request: {'content': ' '}\nMessage received 3.81 seconds after request: {'content': '2'}\nMessage received 3.87 seconds after request: {'content': '0'}\nMessage received 3.91 seconds after request: {'content': ','}\nMessage received 3.97 seconds after request: {'content': ' '}\nMessage received 4.02 seconds after request: {'content': '2'}\nMessage received 4.07 seconds after request: {'content': '1'}\nMessage received 4.07 seconds after request: {'content': ','}\nMessage received 4.12 seconds after request: {'content': ' '}\nMessage received 4.17 seconds after request: {'content': '2'}\nMessage received 4.22 seconds after request: {'content': '2'}\nMessage received 4.27 seconds after request: {'content': ','}\nMessage received 4.33 seconds after request: {'content': ' '}\nMessage received 4.37 seconds after request: {'content': '2'}\nMessage received 4.43 seconds after request: {'content': '3'}\nMessage received 4.43 seconds after request: {'content': ','}\nMessage received 4.48 seconds after request: {'content': ' '}\nMessage received 4.53 seconds after request: {'content': '2'}\nMessage received 4.58 seconds after request: {'content': '4'}\nMessage received 4.63 seconds after request: {'content': ','}\nMessage received 4.68 seconds after request: {'content': ' '}\nMessage received 4.73 seconds after request: {'content': '2'}\nMessage received 4.78 seconds after request: {'content': '5'}\nMessage received 4.83 seconds after request: {'content': ','}\nMessage received 4.89 seconds after request: {'content': ' '}\nMessage received 4.89 seconds after request: {'content': '2'}\nMessage received 4.94 seconds after request: {'content': '6'}\nMessage received 4.99 seconds after request: {'content': ','}\nMessage received 5.04 seconds after request: {'content': ' '}\nMessage received 5.09 seconds after request: {'content': '2'}\nMessage received 5.14 seconds after request: {'content': '7'}\nMessage received 5.20 seconds after request: {'content': ','}\nMessage received 5.25 seconds after request: {'content': ' '}\nMessage received 5.30 seconds after request: {'content': '2'}\nMessage received 5.30 seconds after request: {'content': '8'}\nMessage received 5.35 seconds after request: {'content': ','}\nMessage received 5.40 seconds after request: {'content': ' '}\nMessage received 5.45 seconds after request: {'content': '2'}\nMessage received 5.50 seconds after request: {'content': '9'}\nMessage received 5.55 seconds after request: {'content': ','}\nMessage received 5.60 seconds after request: {'content': ' '}\nMessage received 5.65 seconds after request: {'content': '3'}\nMessage received 5.70 seconds after request: {'content': '0'}\nMessage received 5.76 seconds after request: {'content': ','}\nMessage received 5.76 seconds after request: {'content': ' '}\nMessage received 5.81 seconds after request: {'content': '3'}\nMessage received 5.86 seconds after request: {'content': '1'}\nMessage received 5.91 seconds after request: {'content': ','}\nMessage received 5.96 seconds after request: {'content': ' '}\nMessage received 6.01 seconds after request: {'content': '3'}\nMessage received 6.06 seconds after request: {'content': '2'}\nMessage received 6.11 seconds after request: {'content': ','}\nMessage received 6.17 seconds after request: {'content': ' '}\nMessage received 6.22 seconds after request: {'content': '3'}\nMessage received 6.22 seconds after request: {'content': '3'}\nMessage received 6.27 seconds after request: {'content': ','}\nMessage received 6.32 seconds after request: {'content': ' '}\nMessage received 6.37 seconds after request: {'content': '3'}\nMessage received 6.42 seconds after request: {'content': '4'}\nMessage received 6.47 seconds after request: {'content': ','}\nMessage received 6.52 seconds after request: {'content': ' '}\nMessage received 6.58 seconds after request: {'content': '3'}\nMessage received 6.58 seconds after request: {'content': '5'}\nMessage received 6.63 seconds after request: {'content': ','}\nMessage received 6.68 seconds after request: {'content': ' '}\nMessage received 6.73 seconds after request: {'content': '3'}\nMessage received 6.78 seconds after request: {'content': '6'}\nMessage received 6.83 seconds after request: {'content': ','}\nMessage received 6.88 seconds after request: {'content': ' '}\nMessage received 6.93 seconds after request: {'content': '3'}\nMessage received 6.98 seconds after request: {'content': '7'}\nMessage received 7.04 seconds after request: {'content': ','}\nMessage received 7.09 seconds after request: {'content': ' '}\nMessage received 7.14 seconds after request: {'content': '3'}\nMessage received 7.14 seconds after request: {'content': '8'}\nMessage received 7.19 seconds after request: {'content': ','}\nMessage received 7.24 seconds after request: {'content': ' '}\nMessage received 7.29 seconds after request: {'content': '3'}\nMessage received 7.34 seconds after request: {'content': '9'}\nMessage received 7.40 seconds after request: {'content': ','}\nMessage received 7.45 seconds after request: {'content': ' '}\nMessage received 7.50 seconds after request: {'content': '4'}\nMessage received 7.55 seconds after request: {'content': '0'}\nMessage received 7.55 seconds after request: {'content': ','}\nMessage received 7.60 seconds after request: {'content': ' '}\nMessage received 7.65 seconds after request: {'content': '4'}\nMessage received 7.71 seconds after request: {'content': '1'}\nMessage received 7.75 seconds after request: {'content': ','}\nMessage received 7.80 seconds after request: {'content': ' '}\nMessage received 7.86 seconds after request: {'content': '4'}\nMessage received 7.91 seconds after request: {'content': '2'}\nMessage received 7.96 seconds after request: {'content': ','}\nMessage received 7.96 seconds after request: {'content': ' '}\nMessage received 8.02 seconds after request: {'content': '4'}\nMessage received 8.06 seconds after request: {'content': '3'}\nMessage received 8.11 seconds after request: {'content': ','}\nMessage received 8.17 seconds after request: {'content': ' '}\nMessage received 8.22 seconds after request: {'content': '4'}\nMessage received 8.27 seconds after request: {'content': '4'}\nMessage received 8.32 seconds after request: {'content': ','}\nMessage received 8.37 seconds after request: {'content': ' '}\nMessage received 8.37 seconds after request: {'content': '4'}\nMessage received 8.42 seconds after request: {'content': '5'}\nMessage received 8.47 seconds after request: {'content': ','}\nMessage received 8.52 seconds after request: {'content': ' '}\nMessage received 8.58 seconds after request: {'content': '4'}\nMessage received 8.62 seconds after request: {'content': '6'}\nMessage received 8.68 seconds after request: {'content': ','}\nMessage received 8.73 seconds after request: {'content': ' '}\nMessage received 8.78 seconds after request: {'content': '4'}\nMessage received 8.83 seconds after request: {'content': '7'}\nMessage received 8.88 seconds after request: {'content': ','}\nMessage received 8.88 seconds after request: {'content': ' '}\nMessage received 8.93 seconds after request: {'content': '4'}\nMessage received 8.98 seconds after request: {'content': '8'}\nMessage received 9.04 seconds after request: {'content': ','}\nMessage received 9.09 seconds after request: {'content': ' '}\nMessage received 9.14 seconds after request: {'content': '4'}\nMessage received 9.19 seconds after request: {'content': '9'}\nMessage received 9.24 seconds after request: {'content': ','}\nMessage received 9.29 seconds after request: {'content': ' '}\nMessage received 9.29 seconds after request: {'content': '5'}\nMessage received 9.34 seconds after request: {'content': '0'}\nMessage received 9.40 seconds after request: {}\nFull response received 9.40 seconds after request\nFull conversation received:   Sure! Here it is:\n\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50\n</pre>"},{"location":"examples/stream-chat-completions/#how-to-stream-chat-completion-requests","title":"How to stream Chat Completion requests\u00b6","text":"<p>By default, when you request a completion, the entire completion is generated before being sent back in a single response.</p> <p>If you're generating long completions, waiting for the response can take many seconds.</p> <p>To get responses sooner, you can 'stream' the completion as it's being generated. This allows you to start printing or processing the beginning of the completion before the full completion is finished.</p> <p>To stream completions, set <code>stream=True</code> when calling the chat completions or completions endpoints. This will return an object that streams back the response as data-only server-sent events. Extract chunks from the <code>delta</code> field rather than the <code>message</code> field.</p>"},{"location":"examples/stream-chat-completions/#downsides","title":"Downsides\u00b6","text":"<p>Note that using <code>stream=True</code> in a production application makes it more difficult to moderate the content of the completions, as partial completions may be more difficult to evaluate.</p>"},{"location":"examples/stream-chat-completions/#example-code","title":"Example code\u00b6","text":"<p>Below, this notebook shows:</p> <ol> <li>What a typical chat completion response looks like</li> <li>What a streaming chat completion response looks like</li> <li>How much time is saved by streaming a chat completion</li> </ol>"},{"location":"examples/stream-chat-completions/#1-what-a-typical-chat-completion-response-looks-like","title":"1. What a typical chat completion response looks like\u00b6","text":"<p>With a typical ChatCompletions API call, the response is first computed and then returned all at once.</p>"},{"location":"examples/stream-chat-completions/#2-how-to-stream-a-chat-completion","title":"2. How to stream a chat completion\u00b6","text":"<p>With a streaming API call, the response is sent back incrementally in chunks via an event stream. In Python, you can iterate over these events with a <code>for</code> loop.</p> <p>Let's see what it looks like:</p>"},{"location":"examples/stream-chat-completions/#3-how-much-time-is-saved-by-streaming-a-chat-completion","title":"3. How much time is saved by streaming a chat completion\u00b6","text":"<p>Now let's ask <code>meta-llama/Llama-2-70b-chat-hf</code> to count to 100 again, and see how long it takes.</p>"},{"location":"examples/stream-chat-completions/#time-comparison","title":"Time comparison\u00b6","text":"<p>In the example above, both requests took about 3 seconds to fully complete. Request times will vary depending on load and other stochastic factors.</p> <p>However, with the streaming request, we received the first token after 0.1 seconds, and subsequent tokens every ~0.01-0.02 seconds.</p>"},{"location":"examples/stream-text-completions/","title":"How to stream Text Completion requests","text":"In\u00a0[1]: Copied! <pre># imports\nimport easyllm  # for API calls\nimport time  # for measuring time duration of API calls\n</pre> # imports import easyllm  # for API calls import time  # for measuring time duration of API calls In\u00a0[1]: Copied! <pre>from easyllm.clients import huggingface\nfrom easyllm.prompt_utils import build_llama2_prompt\nhuggingface.prompt_builder = build_llama2_prompt\n\n# a ChatCompletion request\nresponse = huggingface.Completion.create(\n    model=\"meta-llama/Llama-2-70b-chat-hf\",\n    prompt=\"Count to 50, with a comma between each number and no newlines. E.g., 1, 2, 3, ...\",\n    stream=True  # this time, we set stream=True\n)\n\nfor chunk in response:\n    print(chunk)\n</pre> from easyllm.clients import huggingface from easyllm.prompt_utils import build_llama2_prompt huggingface.prompt_builder = build_llama2_prompt  # a ChatCompletion request response = huggingface.Completion.create(     model=\"meta-llama/Llama-2-70b-chat-hf\",     prompt=\"Count to 50, with a comma between each number and no newlines. E.g., 1, 2, 3, ...\",     stream=True  # this time, we set stream=True )  for chunk in response:     print(chunk) <pre>{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' ', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' Sure', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '!', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' Here', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' it', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' is', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ':', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '\\n', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '\\n', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '1', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ',', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' ', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '2', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ',', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' ', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '3', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ',', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' ', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '4', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ',', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' ', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '5', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ',', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' ', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '6', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ',', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' ', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '7', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ',', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' ', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '8', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ',', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' ', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '9', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ',', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' ', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '1', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '0', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ',', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' ', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '1', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '1', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ',', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' ', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '1', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '2', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ',', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' ', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '1', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '3', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ',', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' ', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '1', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '4', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ',', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' ', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '1', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '5', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ',', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' ', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '1', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '6', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ',', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' ', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '1', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '7', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ',', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' ', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '1', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '8', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ',', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' ', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '1', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '9', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ',', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' ', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '2', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '0', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ',', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' ', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '2', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '1', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ',', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' ', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '2', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '2', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ',', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' ', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '2', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '3', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ',', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' ', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '2', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '4', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ',', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' ', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '2', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '5', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ',', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' ', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '2', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '6', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ',', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' ', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '2', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '7', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ',', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' ', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '2', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '8', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ',', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' ', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '2', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '9', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ',', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' ', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '3', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '0', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ',', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' ', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '3', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '1', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ',', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' ', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '3', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '2', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ',', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' ', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '3', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '3', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ',', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' ', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '3', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '4', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ',', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' ', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '3', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '5', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ',', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' ', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '3', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '6', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ',', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' ', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '3', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '7', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ',', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' ', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '3', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '8', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ',', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' ', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '3', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '9', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ',', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' ', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '4', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '0', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ',', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' ', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '4', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '1', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ',', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' ', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '4', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '2', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ',', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' ', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '4', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '3', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ',', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' ', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '4', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '4', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ',', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' ', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '4', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '5', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ',', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' ', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '4', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '6', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ',', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' ', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '4', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '7', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ',', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' ', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '4', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '8', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ',', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' ', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '4', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '9', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ',', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': ' ', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '5', 'logprobs': 0.0}]}\n{'id': 'hf-ag7to4gy5M', 'object': 'text.completion', 'created': 1690987194, 'model': 'meta-llama/Llama-2-70b-chat-hf', 'choices': [{'index': 0, 'text': '0', 'logprobs': 0.0}]}\n</pre> <p>As you can see above, streaming responses have a <code>text</code> field which holds the generated tokens.</p> <p>Below is an example where we print the generated text as it comes in, and stop when we see a <code>&lt;/s&gt;</code> token.</p> In\u00a0[3]: Copied! <pre>from easyllm.clients import huggingface\nfrom easyllm.prompt_utils import build_llama2_prompt\nhuggingface.prompt_builder = build_llama2_prompt\n\n# a ChatCompletion request\nresponse = huggingface.Completion.create(\n    model=\"meta-llama/Llama-2-70b-chat-hf\",\n    prompt=\"Count to 10, with a comma between each number and no newlines. E.g., 1, 2, 3, ...\",\n    stream=True  # this time, we set stream=True\n)\n\nfor chunk in response:\n    print(chunk[\"choices\"][0][\"text\"],end=\"\")\n</pre> from easyllm.clients import huggingface from easyllm.prompt_utils import build_llama2_prompt huggingface.prompt_builder = build_llama2_prompt  # a ChatCompletion request response = huggingface.Completion.create(     model=\"meta-llama/Llama-2-70b-chat-hf\",     prompt=\"Count to 10, with a comma between each number and no newlines. E.g., 1, 2, 3, ...\",     stream=True  # this time, we set stream=True )  for chunk in response:     print(chunk[\"choices\"][0][\"text\"],end=\"\") <pre>  Sure! Here it is: 1, 2, 3, 4, 5, 6, 7, 8, 9, 10.</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"examples/stream-text-completions/#how-to-stream-text-completion-requests","title":"How to stream Text Completion requests\u00b6","text":"<p>By default, when you request a completion, the entire completion is generated before being sent back in a single response.</p> <p>If you're generating long completions, waiting for the response can take many seconds.</p> <p>To get responses sooner, you can 'stream' the completion as it's being generated. This allows you to start printing or processing the beginning of the completion before the full completion is finished.</p> <p>To stream completions, set <code>stream=True</code> when calling the chat completions or completions endpoints. This will return an object that streams back the response as data-only server-sent events. Extract chunks from the <code>delta</code> field rather than the <code>message</code> field.</p>"},{"location":"examples/stream-text-completions/#downsides","title":"Downsides\u00b6","text":"<p>Note that using <code>stream=True</code> in a production application makes it more difficult to moderate the content of the completions, as partial completions may be more difficult to evaluate.</p>"},{"location":"examples/stream-text-completions/#example-code","title":"Example code\u00b6","text":"<p>Below, this notebook shows:</p> <ol> <li>What a streaming text completion response looks like</li> </ol>"},{"location":"examples/stream-text-completions/#2-how-to-stream-a-text-completion","title":"2. How to stream a text completion\u00b6","text":"<p>With a streaming API call, the response is sent back incrementally in chunks via an event stream. In Python, you can iterate over these events with a <code>for</code> loop.</p> <p>Let's see what it looks like:</p>"},{"location":"examples/text-completion-api/","title":"# How to use Text (Instruction) Completion clients","text":"In\u00a0[\u00a0]: Copied! <pre># if needed, install and/or upgrade to the latest version of the OpenAI Python library\n%pip install --upgrade easyllm\n</pre> # if needed, install and/or upgrade to the latest version of the OpenAI Python library %pip install --upgrade easyllm  In\u00a0[6]: Copied! <pre># import the EasyLLM Python library for calling the EasyLLM API\nimport easyllm\n</pre> # import the EasyLLM Python library for calling the EasyLLM API import easyllm In\u00a0[1]: Copied! <pre>from easyllm.clients import huggingface\nfrom easyllm.prompt_utils import build_llama2_prompt\n# Example EasyLLM Python library request\nMODEL = \"meta-llama/Llama-2-70b-chat-hf\"\nhuggingface.prompt_builder = build_llama2_prompt\n\n# The module automatically loads the HuggingFace API key from the environment variable HUGGINGFACE_TOKEN or from the HuggingFace CLI configuration file.\n# huggingface.api_key=\"hf_xxx\"\n\nresponse = huggingface.Completion.create(\n    model=MODEL,\n    prompt=\"What is the meaning of life?\",\n    temperature=0.9,\n    top_p=0.6,\n    max_tokens=1024,\n)\nresponse\n</pre>  from easyllm.clients import huggingface from easyllm.prompt_utils import build_llama2_prompt # Example EasyLLM Python library request MODEL = \"meta-llama/Llama-2-70b-chat-hf\" huggingface.prompt_builder = build_llama2_prompt  # The module automatically loads the HuggingFace API key from the environment variable HUGGINGFACE_TOKEN or from the HuggingFace CLI configuration file. # huggingface.api_key=\"hf_xxx\"  response = huggingface.Completion.create(     model=MODEL,     prompt=\"What is the meaning of life?\",     temperature=0.9,     top_p=0.6,     max_tokens=1024, ) response Out[1]: <pre>{'id': 'hf-bgZcyBsDW_',\n 'object': 'text.completion',\n 'created': 1690986829,\n 'model': 'meta-llama/Llama-2-70b-chat-hf',\n 'choices': [{'index': 0,\n   'text': \" The meaning of life is a question that has puzzled philosophers, theologians, and scientists for centuries. There are many different perspectives on what constitutes the meaning of life, and there is no one definitive answer. However, some common themes that people often associate with the meaning of life include:\\n\\n1. Purpose: Having a sense of purpose or direction in life, whether it be through work, relationships, or personal goals.\\n2. Fulfillment: Feeling fulfilled and satisfied with one's experiences and achievements.\\n3. Happiness: Pursuing happiness and well-being, whether through personal relationships, material possessions, or personal growth.\\n4. Legacy: Leaving a lasting impact or legacy, whether through contributions to society, artistic or cultural achievements, or the impact one has on others.\\n5. Spirituality: Connecting with a higher power or a sense of something greater than oneself, whether through religion, meditation, or personal beliefs.\\n6. Personal growth: Continuously learning, growing, and improving oneself.\\n7. Love: Experiencing and expressing love for oneself, others, and the world around us.\\n8. Community: Building and being a part of a community, whether through family, friends, or social connections.\\n9. Contribution: Making a positive impact in the world and contributing to the greater good.\\n10. Meaningful experiences: Having experiences that are meaningful and memorable, whether through travel, personal achievements, or time with loved ones.\\n\\nUltimately, the meaning of life is a deeply personal and subjective question, and what gives meaning and purpose to one person's life may be different for another. It's a question that each person must answer for themselves, and it may change throughout their life as they grow and evolve.\",\n   'finish_reason': 'eos_token'}],\n 'usage': {'prompt_tokens': 11, 'completion_tokens': 406, 'total_tokens': 417}}</pre> <p>As you can see, the response object has a few fields:</p> <ul> <li><code>id</code>: the ID of the request</li> <li><code>object</code>: the type of object returned (e.g., <code>text.completion</code>)</li> <li><code>created</code>: the timestamp of the request</li> <li><code>model</code>: the full name of the model used to generate the response</li> <li><code>usage</code>: the number of tokens used to generate the replies, counting prompt, completion, and total</li> <li><code>choices</code>: a list of completion objects (only one, unless you set <code>n</code> greater than 1)<ul> <li><code>text</code>: the generated text</li> <li><code>finish_reason</code>: the reason the model stopped generating text (either <code>stop</code>, <code>eos_token</code>, or <code>length</code> if <code>max_tokens</code> limit was reached)</li> <li><code>logprobs</code>: optional the log probs of each generated token.</li> </ul> </li> </ul> <p>Extract just the reply with:</p> In\u00a0[3]: Copied! <pre>print(response['choices'][0]['text'])\n</pre> print(response['choices'][0]['text']) <pre> The meaning of life is a question that has puzzled philosophers, theologians, and scientists for centuries. There are many different perspectives on what constitutes the meaning of life, and there is no one definitive answer. However, some common themes that people often associate with the meaning of life include:\n\n1. Purpose: Having a sense of purpose or direction in life, whether it be through work, relationships, or personal goals.\n2. Fulfillment: Feeling fulfilled and satisfied with one's experiences and achievements.\n3. Happiness: Pursuing happiness and well-being, whether through personal relationships, material possessions, or personal growth.\n4. Legacy: Leaving a lasting impact or legacy, whether through contributions to society, artistic or cultural achievements, or the impact one has on others.\n5. Spirituality: Connecting with a higher power or a sense of something greater than oneself, whether through religion, meditation, or personal beliefs.\n6. Personal growth: Continuously learning, growing, and improving oneself.\n7. Love: Experiencing and expressing love for oneself, others, and the world around us.\n8. Community: Building and being a part of a community, whether through family, friends, or social connections.\n9. Contribution: Making a positive impact in the world and contributing to the greater good.\n10. Meaningful experiences: Having experiences that are meaningful and memorable, whether through travel, personal achievements, or time with loved ones.\n\nUltimately, the meaning of life is a deeply personal and subjective question, and what gives meaning and purpose to one person's life may be different for another. It's a question that each person must answer for themselves, and it may change throughout their life as they grow and evolve.\n</pre>"},{"location":"examples/text-completion-api/#how-to-use-text-instruction-completion-clients","title":"# How to use Text (Instruction) Completion clients\u00b6","text":"<p>EasyLLM can be used as an abstract layer to replace <code>text-davinci-003</code> with open source models.</p> <p>You can change your own applications from the OpenAI API, by simply changing the client.</p> <p>Chat models take a series of messages as input, and return an AI-written message as output.</p> <p>This guide illustrates the chat format with a few example API calls.</p>"},{"location":"examples/text-completion-api/#1-import-the-easyllm-library","title":"1. Import the easyllm library\u00b6","text":""},{"location":"examples/text-completion-api/#2-an-example-chat-api-call","title":"2. An example chat API call\u00b6","text":"<p>A text API call has two required inputs:</p> <ul> <li><code>model</code>: the name of the model you want to use (e.g., <code>meta-llama/Llama-2-70b-chat-hf</code>) or leave it empty to just call the api</li> <li><code>prompt</code>: a text prompt that is sent to the model to generate the text</li> </ul> <p>Compared to OpenAI api is the <code>huggingface</code> module also exposing a <code>prompt_builder</code> and <code>stop_sequences</code> parameter you can use to customize the prompt and stop sequences. The EasyLLM package comes with build in popular methods for both of these parameters, e.g. <code>llama2_prompt_builder</code> and <code>llama2_stop_sequences</code>.</p> <p>Let's look at an example chat API calls to see how the chat format works in practice.</p>"}]}